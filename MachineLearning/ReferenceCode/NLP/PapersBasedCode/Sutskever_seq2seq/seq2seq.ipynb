{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site: https://paperswithcode.com/ contains code reference for published papers. The original ref. for this code is from https://github.com/bentrevett/pytorch-seq2seq which was referenced in the paperswithcode.com link.\n",
    "\n",
    "The code implements the paper \"Sequence to Sequence Learning with Neural Networks\", Ilya Sutskever, Oriol Vinyals, Quoc V. Le, available at: https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm # to display a progress bar\n",
    "import evaluate\n",
    "\n",
    "#import import_ipynb # Give it before import of other ipynb\n",
    "#from Encoder import Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed in all libraries so that startup weights and other paramters are same in every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 36\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used here is the English to German translation text. Orig. data set is available here: https://github.com/multi30k/dataset. The HF datasets library has access to this under \"bentrevett/multi30k\". We will load this from HF.\n",
    "\n",
    "This Dataset is already split into  Training, Validation and Test groups like most of the datasets in the HF library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "dataset # Print the data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign Train, Test and Validation into variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are sample contents: train_data[0]\n",
    "\n",
    "{'en': 'Two young, White males are outside near many bushes.',\n",
    " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tokenization we will use spaCy which is a newer library than NLTK and tiktoken. We first need to download the tokenization models for each language which will be en and de in this case.\n",
    "\n",
    "Unlike NLTK, there is no way to download the models from code in Spacy, hence the following commands have to be run before loading the tokenizations models.\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "\n",
    "These models are downloaded in the folder:\n",
    ".venv\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-3.7.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\PapersBasedCode\\.venv\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-3.7.1\n"
     ]
    }
   ],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Print download path\n",
    "print(en_nlp._path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us manually call the Tokenizer for Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Top', 'Gun', 'is', 'my', 'favorite', 'movie', '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Top Gun is my favorite movie!\"\n",
    "tokens = en_nlp.tokenizer(string)\n",
    "\n",
    "[token.text for token in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to be used by the map method of the Datasets objects. Add the Start of sentence (sos) and the End of Sentence (eos) tokens which are passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arguments can be passed as a kwargs dict.\n",
    "def tokenize_for_map(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    # max_length will terminate the string if longer than a specific length\n",
    "    # this step is reapeated for each token\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    # Return as a dict.\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters to pass to tokenize_for_map() as kw_args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call tokenize_for_map() with above arguements for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(tokenize_for_map, fn_kwargs=kwargs)\n",
    "valid_data = valid_data.map(tokenize_for_map, fn_kwargs=kwargs)\n",
    "test_data = test_data.map(tokenize_for_map, fn_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See a sample of the data after the tokenize_for_map() operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the vocabulary now which is assigning unique token_ids to each token, which serves as a lookup table mapping numbers to tokens. We also assign the Unknown '<unk>' and Pad '<pad>' token. \n",
    "\n",
    "The special_tokens variables is set to a list that will be passed to the torchtext.vocab.build_vocab_from_iterator()\n",
    "\n",
    "The min_freq param specifies that only tokens who appear min_freq times should be considered in the dataset. If any token is less than min_freq times it will be treated a <unk> token.  \n",
    "\n",
    "The parameters to create the vocab. are specified first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call the functions to build both the 'en' and 'de' vocabularies. vocab should only be built from training data, if some token is present in test/validation but in training, then it should be treated as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a sample of the vocab. Print the 1st 10 tokens in the vocab. The regular tokens are orderded from the most frequenct to least frequent, and the specical tokens are not subject to this. itos() shows the string for a given index while stoi() will give the index for a specific string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man'],\n",
       " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ','])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: In .ipynb only the last line is printed\n",
    "en_vocab.get_itos()[:10], de_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some stoi() values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_stoi()[\"my\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also use the object as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"my\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index of special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[unk_token], en_vocab[pad_token], de_vocab[unk_token], de_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look up indices of multiple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1916, 1507, 10, 7, 9]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"my\",\"name\",\"is\",\"the\",\"man\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get size of the vocabulary in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5893, 7853)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_vocab), len(de_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab. can be used like a Map and similar operations can be performed on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special tokens will have the same id and using an Assert we can confirm that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default index. The default index is returned if some token is not found in the vocab. This is a very important step, as you will get 'key' not found errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(en_vocab.get_stoi()[unk_token])\n",
    "de_vocab.set_default_index(en_vocab.get_stoi()[unk_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will return the indices for any passed group of string and will behave in a similar way like we call map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function let's add the token ids to the train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "# Add the en_ids and de_ids rows\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check one row and see the additional column that is added. 'en_ids' and 'de_ids' contain the ids (or token ids) of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'en_ids': [2, 16, 24, 15, 25, 778, 17, 57, 80, 202, 1312, 5, 3],\n",
       " 'de_ids': [2, 18, 26, 253, 30, 84, 20, 88, 7, 15, 110, 7647, 3171, 4, 3]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert specific columns of en_ids and de_ids to torch tensors.We will use the with_format() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=\"torch\", columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(type=\"torch\",\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(type=\"torch\",\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch's  DataLoader class will be used to create the batch. Dataloader can call special functions when creating batches, and we will use a collate (Combine) function to pad the input sequence. Padding of input sequence is important as the matrix for weights is fixed. pad_sequence() of nn.utils will be used to pad the sequence.\n",
    "\n",
    "Note that we use a closure type construct for get_collate_fn(). When DataLoader calls the collage function it only sends it the batch, as function param and not the pad_index. By using the function within a function the pad_index value needs to be passed once, and then collate_fn() can be called directly and will use the pad_index value defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_collate_fn(pad_index): # This () is called once, assigns value of pad_index\n",
    "    def collate_fn(batch):     # Called by the dataloader.\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the get_data_loader() that will call the collate(). This will return the DataLoader objects. This function is called multiple times for train, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index) # Get collate_fn as a value, same pad_index will be used\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call get_data_loader() for train, test and validation. Set to a high batchs size, if GPU is available use the largest batch size that will fit in GPU memory. For training, data should be shuffled but not needed for test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to build the model. Model will be built in 3 parts, the Encoder, Decoder then the seq2seq linkage between the two. Note that this uses the nn.Embedding layer of pytorch, so embeddings weights will also be learned (we are not using any pretrained embeddings like word2vec). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        print(f\"Input dim {input_dim}, embedding dim {embedding_dim}\")\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # LSTM(# input size, #hidden, # layers), # hidden also corresponds to output count \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout) # dropout = prob. of dropout (for randomly zeroing the input tensor values)\n",
    "\n",
    "\n",
    "    # src is the en_ids, or list of token ids in a sentence\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # LSTM Input:\n",
    "        # input: tensor of shape  (L,Hin) for unbatched input,  (L,N,Hin)  when batch_first=False or  \n",
    "        # (N,L,Hin) when batch_first=True containing the features of the input sequence.\n",
    "        # L = src length, N = batch size, Hin = embedded dimension\n",
    "        #\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        # No need to pass hidden, cell states \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # LSTM Output format:  output, (h_n, c_n)\n",
    "        # output: Output features for each t \n",
    "        # h_n: final hidden state for each LSTM Cell\n",
    "        # c_n: final cell state for each LSTM Cell\n",
    "        #\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top/last hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for Decoder is shown next. This has a FC layer in the end to allow for a softmax like output to predict the probabilites of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim) # 1\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) #2\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim) # 3: To make predictions for next token\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size] # seq length is 1  \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        # initial hidden, cell state is passed from encoder\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        # after sqeeze output: [batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0)) # remove all dimensions of size 0\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq model is built now. It will use both the encoder and the decoder. Encoder will generate the context vector while decover will be used to generate the target. \n",
    "\n",
    "Output from each hidden layer in the Encoder will be fed to the Decoder. Hence for simplicity, the number of layers need to be same in Encoder and Decoder. However, the layers can be different by using average of layers etc.. For example if Encoder has 2 layers and Decoder 1, then the average of 2 layers can be taken to reduce the dimensionality to 1, and then pass the values on. Similarly, both Encoder and decoder should have same number of dimensions.\n",
    "\n",
    "The routine also used teacher forcing. Teacher forcing inserts the actual token (ground truth) and not the predicted token. Teacher forcing is controlled by a probability thereshold, which determines if the next token should be given by prediction or the actual value should be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        print(\"src size:\", src.size())\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size] # target\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            # hidden, cell are from the output\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using device: cuda\n",
      "Input dim 7853, embedding dim 256\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(de_vocab)  # de_vocab stoes token_ids\n",
    "output_dim = len(en_vocab) # en_vocab[\"my\"] = 1916, stores token ids\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"*** Using device:\",device)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights, using a uniform distribution. Also define a count_params function that will count the number of params in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "# Using required_grad ensures only trainable params are used in the count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the main training function, this will take in model, optimizer, loss function and other details and execute the training loop. The training loop is called for each epoch. Returns the avg. epoch_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"de_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        # Send the token ids to the model\n",
    "        # This will run the seq2seq model\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1] # Get the value from last index\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate function to evaluate the results. Similar to training, but with model.eval() enabled and no use of optim function. Return the avg. epoch loss for each batch, will be called for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the model training. This will be the orchestrator that will call all other prev. defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([22, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([46, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([41, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 72])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:02<09:22, 62.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src size: torch.Size([33, 118])\n",
      "\tTrain Loss:   5.049 | Train PPL: 155.862\n",
      "\tValid Loss:   4.966 | Valid PPL: 143.498\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([41, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([22, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([46, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([23, 72])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 118])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:04<08:18, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.478 | Train PPL:  88.059\n",
      "\tValid Loss:   4.770 | Valid PPL: 117.976\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([35, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([41, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([38, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([45, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([46, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([33, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([34, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([37, 128])\n",
      "src size: torch.Size([27, 128])\n",
      "src size: torch.Size([29, 128])\n",
      "src size: torch.Size([31, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([26, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([24, 128])\n",
      "src size: torch.Size([32, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([23, 128])\n",
      "src size: torch.Size([30, 128])\n",
      "src size: torch.Size([36, 128])\n",
      "src size: torch.Size([25, 128])\n",
      "src size: torch.Size([28, 128])\n",
      "src size: torch.Size([29, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:51<11:25, 85.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# tqdm will auto display a progress bar\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[1;32m---> 13\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate_fn(\n\u001b[0;32m     23\u001b[0m         model,\n\u001b[0;32m     24\u001b[0m         valid_data_loader,\n\u001b[0;32m     25\u001b[0m         criterion,\n\u001b[0;32m     26\u001b[0m         device,\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[72], line 25\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 25\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# model is already defined before in line 55\n",
    "# batch_size is define in line 51\n",
    "\n",
    "print(f\"Batch Size {batch_size}\")\n",
    "# tqdm will auto display a progress bar\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut1-model.pt\") # Save the model with best loss\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
