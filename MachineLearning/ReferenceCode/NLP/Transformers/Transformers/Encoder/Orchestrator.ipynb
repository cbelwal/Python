{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to use\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset with HF library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sst2 dataset of the glue benchmark\n",
    "# sst2 is used for sentiment analysis and will have a target label \n",
    "# raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets = load_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns not needed\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and validation test sets and set the batch size\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how it works\n",
    "for batch in train_loader:\n",
    "  for k, v in batch.items():\n",
    "    print(\"k:\", k, \"v.shape:\", v.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to set: unordered collection of unique elements\n",
    "set(tokenized_datasets['train']['labels'])\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '_' is allowed between number for easier readibility\n",
    "    vocab_size,max_len,d_k, d_model, n_heads,n_layers,n_classes,dropout_prob\n",
    "    vocab_size = 20,000\n",
    "    max_len = 1024\n",
    "    d_k = 16\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    n_layers = 2\n",
    "    n_classes = 5\n",
    "    dropout_prob = 0.1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Encoder import Encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "model = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=512, #tokenizer.max_model_input_sizes[checkpoint],\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    n_classes=2,\n",
    "    dropout_prob=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See samples of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In BERT token ids 101 and 102 represent start and end of sentence respectively \n",
    "\"\"\"\n",
    "for batch in train_loader:\n",
    "    # modify batch size to 1 before running this\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(batch)\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = 0\n",
    "    n_train = 0\n",
    "    for batch in train_loader:\n",
    "      # move data to GPU\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      # model forward function only takes two params (the attention is optional) \n",
    "      # and the output is a binary classification problem\n",
    "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "      loss = criterion(outputs, batch['labels'])\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()*batch['input_ids'].size(0)\n",
    "      n_train += batch['input_ids'].size(0)\n",
    "\n",
    "    # Get average train loss\n",
    "    train_loss = train_loss / n_train\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_test = 0\n",
    "    for batch in valid_loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "      loss = criterion(outputs, batch['labels'])\n",
    "      test_loss += loss.item()*batch['input_ids'].size(0)\n",
    "      n_test += batch['input_ids'].size(0)\n",
    "    test_loss = test_loss / n_test\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "      Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "  \n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "train_losses, test_losses = train(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, epochs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
