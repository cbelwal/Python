{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from Decoder import Decoder\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersCustom\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInferTokenIds(model, input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "    # logits will contain probabilities for each token    \n",
    "    print(\"Output shape:\", output.shape) # torch.Size([1, 12, 43])\n",
    "\n",
    "    # output contains the logits\n",
    "    # get the index for the highest logits for each token\n",
    "    predictionIdxs = torch.argmax(output, axis=-1)\n",
    "    print(\"Prediction Idxs shape:\", predictionIdxs.shape) # torch.Size([1, 12])\n",
    "    \n",
    "    # Convert to list\n",
    "    predictionIds = predictionIdxs.squeeze(0).tolist()\n",
    "    # Get token ids from idx\n",
    "    #predTokenIds = customTokenizer.getTokenIdsForIdxs(predictionIdxs)\n",
    "    return predictionIds\n",
    "     \n",
    "def getDecodedSentence(inputTokens):\n",
    "    return customTokenizer.decodeTokenizedSentence(inputTokens)\n",
    "\n",
    "def runInferenceTillEnd(model, startTokens):\n",
    "    input = customTokenizer.encodeTokenizedSentence(startTokens) # will add start and end tokens\n",
    "    input = torch.tensor(input).unsqueeze(0).to(device)\n",
    "    predTokenIds = getInferTokenIds(model, input)\n",
    "    return getDecodedSentence(predTokenIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 12, 43])\n",
      "Prediction Idxs shape: torch.Size([1, 12])\n",
      "the what for what high high high high high high high high high\n"
     ]
    }
   ],
   "source": [
    "def testInfer_1():\n",
    "    # Check inference with current model\n",
    "    startTokens = \"the\"\n",
    "    print(startTokens + \" \" + runInferenceTillEnd(model,startTokens)) # All are lower case\n",
    "\n",
    "testInfer_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "    rowsTrain = trainDataTensor.shape[0]\n",
    "    for i in range(rowsTrain):\n",
    "      #print(f\"{i}/{rowsTrain}\")\n",
    "      x_t = trainDataTensor[i].unsqueeze(0).to(device)\n",
    "     \n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "      # print(\"outputs:\", outputs)\n",
    "      # print(\"targets:\", targets)\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optim and criterion\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId())\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 2.0687, Duration: 0:06:49.859084\n",
      "Epoch 2/100, Train Loss: 2.0356, Duration: 0:00:01.170999\n",
      "Epoch 3/100, Train Loss: 2.0491, Duration: 0:00:01.158245\n",
      "Epoch 4/100, Train Loss: 2.0516, Duration: 0:00:01.147887\n",
      "Epoch 5/100, Train Loss: 2.0496, Duration: 0:00:01.204300\n",
      "Epoch 6/100, Train Loss: 2.0350, Duration: 0:00:01.148643\n",
      "Epoch 7/100, Train Loss: 2.0538, Duration: 0:00:01.087868\n",
      "Epoch 8/100, Train Loss: 2.0529, Duration: 0:00:01.125780\n",
      "Epoch 9/100, Train Loss: 2.0523, Duration: 0:00:01.132392\n",
      "Epoch 10/100, Train Loss: 2.0489, Duration: 0:00:01.060945\n",
      "Epoch 11/100, Train Loss: 2.0463, Duration: 0:00:01.082432\n",
      "Epoch 12/100, Train Loss: 2.0333, Duration: 0:00:01.154179\n",
      "Epoch 13/100, Train Loss: 2.0403, Duration: 0:00:01.118678\n",
      "Epoch 14/100, Train Loss: 2.0498, Duration: 0:00:01.069520\n",
      "Epoch 15/100, Train Loss: 2.0374, Duration: 0:00:01.095247\n",
      "Epoch 16/100, Train Loss: 2.0452, Duration: 0:00:01.115371\n",
      "Epoch 17/100, Train Loss: 2.0287, Duration: 0:00:01.143713\n",
      "Epoch 18/100, Train Loss: 2.0597, Duration: 0:00:01.152287\n",
      "Epoch 19/100, Train Loss: 2.0409, Duration: 0:00:01.068726\n",
      "Epoch 20/100, Train Loss: 2.0389, Duration: 0:00:01.084411\n",
      "Epoch 21/100, Train Loss: 2.0396, Duration: 0:00:01.118872\n",
      "Epoch 22/100, Train Loss: 2.0360, Duration: 0:00:01.092085\n",
      "Epoch 23/100, Train Loss: 2.0335, Duration: 0:00:01.078655\n",
      "Epoch 24/100, Train Loss: 2.0356, Duration: 0:00:01.101877\n",
      "Epoch 25/100, Train Loss: 2.0679, Duration: 0:00:01.085546\n",
      "Epoch 26/100, Train Loss: 2.0361, Duration: 0:00:01.068666\n",
      "Epoch 27/100, Train Loss: 2.0189, Duration: 0:00:01.141038\n",
      "Epoch 28/100, Train Loss: 2.0398, Duration: 0:00:01.135620\n",
      "Epoch 29/100, Train Loss: 2.0141, Duration: 0:00:01.172575\n",
      "Epoch 30/100, Train Loss: 2.0344, Duration: 0:00:01.081691\n",
      "Epoch 31/100, Train Loss: 2.0243, Duration: 0:00:01.102008\n",
      "Epoch 32/100, Train Loss: 2.0331, Duration: 0:00:01.118790\n",
      "Epoch 33/100, Train Loss: 2.0164, Duration: 0:00:01.085366\n",
      "Epoch 34/100, Train Loss: 2.0413, Duration: 0:00:01.085372\n",
      "Epoch 35/100, Train Loss: 2.0375, Duration: 0:00:01.102102\n",
      "Epoch 36/100, Train Loss: 2.0161, Duration: 0:00:01.118727\n",
      "Epoch 37/100, Train Loss: 2.0102, Duration: 0:00:01.117960\n",
      "Epoch 38/100, Train Loss: 2.0225, Duration: 0:00:01.085810\n",
      "Epoch 39/100, Train Loss: 2.0391, Duration: 0:00:01.068286\n",
      "Epoch 40/100, Train Loss: 2.0396, Duration: 0:00:01.152091\n",
      "Epoch 41/100, Train Loss: 2.0284, Duration: 0:00:01.127490\n",
      "Epoch 42/100, Train Loss: 2.0191, Duration: 0:00:01.124255\n",
      "Epoch 43/100, Train Loss: 2.0236, Duration: 0:00:01.103464\n",
      "Epoch 44/100, Train Loss: 2.0103, Duration: 0:00:01.052010\n",
      "Epoch 45/100, Train Loss: 2.0151, Duration: 0:00:01.070149\n",
      "Epoch 46/100, Train Loss: 2.0227, Duration: 0:00:01.097118\n",
      "Epoch 47/100, Train Loss: 2.0071, Duration: 0:00:01.072613\n",
      "Epoch 48/100, Train Loss: 2.0227, Duration: 0:00:01.089549\n",
      "Epoch 49/100, Train Loss: 2.0134, Duration: 0:00:01.081127\n",
      "Epoch 50/100, Train Loss: 2.0219, Duration: 0:00:01.052040\n",
      "Epoch 51/100, Train Loss: 2.0230, Duration: 0:00:01.070598\n",
      "Epoch 52/100, Train Loss: 2.0119, Duration: 0:00:01.083517\n",
      "Epoch 53/100, Train Loss: 2.0079, Duration: 0:00:01.101517\n",
      "Epoch 54/100, Train Loss: 2.0266, Duration: 0:00:01.101977\n",
      "Epoch 55/100, Train Loss: 2.0143, Duration: 0:00:01.112361\n",
      "Epoch 56/100, Train Loss: 2.0014, Duration: 0:00:01.077311\n",
      "Epoch 57/100, Train Loss: 2.0012, Duration: 0:00:01.066449\n",
      "Epoch 58/100, Train Loss: 2.0069, Duration: 0:00:01.085361\n",
      "Epoch 59/100, Train Loss: 2.0139, Duration: 0:00:01.064064\n",
      "Epoch 60/100, Train Loss: 2.0031, Duration: 0:00:01.166719\n",
      "Epoch 61/100, Train Loss: 2.0162, Duration: 0:00:01.110056\n",
      "Epoch 62/100, Train Loss: 2.0057, Duration: 0:00:01.116547\n",
      "Epoch 63/100, Train Loss: 2.0019, Duration: 0:00:01.061683\n",
      "Epoch 64/100, Train Loss: 2.0114, Duration: 0:00:01.202302\n",
      "Epoch 65/100, Train Loss: 1.9968, Duration: 0:00:01.158552\n",
      "Epoch 66/100, Train Loss: 1.9869, Duration: 0:00:01.084900\n",
      "Epoch 67/100, Train Loss: 2.0072, Duration: 0:00:01.102083\n",
      "Epoch 68/100, Train Loss: 1.9984, Duration: 0:00:01.151046\n",
      "Epoch 69/100, Train Loss: 1.9782, Duration: 0:00:01.135957\n",
      "Epoch 70/100, Train Loss: 2.0035, Duration: 0:00:01.141733\n",
      "Epoch 71/100, Train Loss: 2.0000, Duration: 0:00:01.145890\n",
      "Epoch 72/100, Train Loss: 2.0109, Duration: 0:00:01.169388\n",
      "Epoch 73/100, Train Loss: 1.9923, Duration: 0:00:01.151630\n",
      "Epoch 74/100, Train Loss: 2.0049, Duration: 0:00:01.319164\n",
      "Epoch 75/100, Train Loss: 1.9989, Duration: 0:00:01.394694\n",
      "Epoch 76/100, Train Loss: 1.9966, Duration: 0:00:01.142251\n",
      "Epoch 77/100, Train Loss: 2.0072, Duration: 0:00:01.135604\n",
      "Epoch 78/100, Train Loss: 2.0030, Duration: 0:00:01.158070\n",
      "Epoch 79/100, Train Loss: 2.0006, Duration: 0:00:01.112846\n",
      "Epoch 80/100, Train Loss: 2.0032, Duration: 0:00:01.105788\n",
      "Epoch 81/100, Train Loss: 1.9963, Duration: 0:00:01.098419\n",
      "Epoch 82/100, Train Loss: 2.0077, Duration: 0:00:01.168695\n",
      "Epoch 83/100, Train Loss: 1.9881, Duration: 0:00:01.135576\n",
      "Epoch 84/100, Train Loss: 2.0156, Duration: 0:00:01.118859\n",
      "Epoch 85/100, Train Loss: 1.9948, Duration: 0:00:01.084777\n",
      "Epoch 86/100, Train Loss: 1.9948, Duration: 0:00:01.135321\n",
      "Epoch 87/100, Train Loss: 1.9818, Duration: 0:00:01.118905\n",
      "Epoch 88/100, Train Loss: 1.9927, Duration: 0:00:01.110016\n",
      "Epoch 89/100, Train Loss: 1.9806, Duration: 0:00:01.160765\n",
      "Epoch 90/100, Train Loss: 1.9616, Duration: 0:00:01.102061\n",
      "Epoch 91/100, Train Loss: 1.9896, Duration: 0:00:01.112949\n",
      "Epoch 92/100, Train Loss: 1.9724, Duration: 0:00:01.107030\n",
      "Epoch 93/100, Train Loss: 1.9789, Duration: 0:00:01.085414\n",
      "Epoch 94/100, Train Loss: 1.9939, Duration: 0:00:01.085337\n",
      "Epoch 95/100, Train Loss: 1.9825, Duration: 0:00:01.135499\n",
      "Epoch 96/100, Train Loss: 1.9763, Duration: 0:00:01.101894\n",
      "Epoch 97/100, Train Loss: 2.0016, Duration: 0:00:01.118960\n",
      "Epoch 98/100, Train Loss: 2.0023, Duration: 0:00:01.118727\n",
      "Epoch 99/100, Train Loss: 1.9714, Duration: 0:00:01.119274\n",
      "Epoch 100/100, Train Loss: 1.9816, Duration: 0:00:01.151615\n"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInfer_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
