{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from Decoder import Decoder\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersCustom\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 238891\n"
     ]
    }
   ],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = \"it\\'s a\"\\ntokenized_prompt = tokenizer(prompt, return_tensors=\\'pt\\')\\n# prepare inputs + get rid of SEP token at the end\\ninput_ids = tokenized_prompt[\\'input_ids\\'][:, :-1].to(device)\\nmask = tokenized_prompt[\\'attention_mask\\'][:, :-1].to(device)\\nfor _ in range(20):\\n  outputs = model(input_ids, mask)\\n  input_ids = torch.hstack((input_ids, prediction_id.view(1, 1)))\\n  mask = torch.ones_like(input_ids)\\n  if prediction_id == tokenizer.sep_token_id:\\n    break\\n  tokenizer.decode(input_ids[0])\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getInferTokenIds(model, input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        outputs = model(input)\n",
    "    # logits will contain probabilities for each token    \n",
    "    #print(\"Outputs shape:\", outputs.shape) # torch.Size([1, 12, 43])\n",
    "\n",
    "    # output contains the logits\n",
    "    # get the index for the highest logits for each token\n",
    "    #predictionIdxs = torch.argmax(output, axis=-1)\n",
    "    predictionId = torch.argmax(outputs[:, -1, :], axis=-1)\n",
    "    #print(\"Prediction Id shape:\", predictionId.shape) # torch.Size([1, 12])\n",
    "    \n",
    "    # Convert to list\n",
    "    #predictionIds = predictionIdxs.squeeze(0).tolist()\n",
    "    # Get token ids from idx\n",
    "    #predTokenIds = customTokenizer.getTokenIdsForIdxs(predictionIdxs)\n",
    "    return predictionId # return as a tensor\n",
    "     \n",
    "def getDecodedSentence(tensorInputTokens):\n",
    "    # Convert to list\n",
    "    inputTokenIds = tensorInputTokens.squeeze(0).tolist()\n",
    "    return customTokenizer.decode(inputTokenIds)\n",
    "\n",
    "def runInference(model, prompt):\n",
    "    tokenizedPrompt = customTokenizer.encode(prompt) # will add start and end tokens\n",
    "    # Remove the SEP Token at the end\n",
    "    inputTokenIds = tokenizedPrompt[:-1] # Mask is not being considered at this time\n",
    "    tensorInputTokenIds = torch.tensor(inputTokenIds).unsqueeze(0).to(device)\n",
    "    len = 2\n",
    "    while(len < customTokenizer.getMaxLen()):\n",
    "        len += 1\n",
    "        predTokenId = getInferTokenIds(model, tensorInputTokenIds)\n",
    "        tensorInputTokenIds = torch.hstack((tensorInputTokenIds, predTokenId.view(1, 1)))\n",
    "        if predTokenId == customTokenizer.sepTokenId:\n",
    "            break\n",
    "    return getDecodedSentence(tensorInputTokenIds)\n",
    "\n",
    "'''\n",
    "prompt = \"it's a\"\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "# prepare inputs + get rid of SEP token at the end\n",
    "input_ids = tokenized_prompt['input_ids'][:, :-1].to(device)\n",
    "mask = tokenized_prompt['attention_mask'][:, :-1].to(device)\n",
    "for _ in range(20):\n",
    "  outputs = model(input_ids, mask)\n",
    "  input_ids = torch.hstack((input_ids, prediction_id.view(1, 1)))\n",
    "  mask = torch.ones_like(input_ids)\n",
    "  if prediction_id == tokenizer.sep_token_id:\n",
    "    break\n",
    "  tokenizer.decode(input_ids[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Romi\n",
      "<CLS> romi leaped jump in inside not a inside not a inside\n",
      "Response: None\n"
     ]
    }
   ],
   "source": [
    "def testInfer_1():\n",
    "    # Check inference with current model\n",
    "    prompt = \"Romi\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {print(runInference(model,prompt))}\") # All are lower case\n",
    "\n",
    "testInfer_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([154, 12])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "    rowsTrain = trainDataTensor.shape[0]\n",
    "    for i in range(rowsTrain):\n",
    "      #print(f\"{i}/{rowsTrain}\")\n",
    "      x_t = trainDataTensor[i].unsqueeze(0).to(device)\n",
    "     \n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "      # print(\"outputs:\", outputs)\n",
    "      # print(\"targets:\", targets)\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optim and criterion\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId())\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.8032, Duration: 0:00:01.103063\n",
      "Epoch 2/100, Train Loss: 1.3070, Duration: 0:00:01.183914\n",
      "Epoch 3/100, Train Loss: 1.2177, Duration: 0:00:01.125852\n",
      "Epoch 4/100, Train Loss: 1.1614, Duration: 0:00:01.157971\n",
      "Epoch 5/100, Train Loss: 1.1198, Duration: 0:00:01.184624\n",
      "Epoch 6/100, Train Loss: 1.0821, Duration: 0:00:01.381633\n",
      "Epoch 7/100, Train Loss: 1.0744, Duration: 0:00:01.140858\n",
      "Epoch 8/100, Train Loss: 1.0481, Duration: 0:00:01.096581\n",
      "Epoch 9/100, Train Loss: 1.0520, Duration: 0:00:01.185402\n",
      "Epoch 10/100, Train Loss: 1.0595, Duration: 0:00:01.219122\n",
      "Epoch 11/100, Train Loss: 1.0205, Duration: 0:00:01.165521\n",
      "Epoch 12/100, Train Loss: 1.0198, Duration: 0:00:01.170201\n",
      "Epoch 13/100, Train Loss: 1.0042, Duration: 0:00:01.102082\n",
      "Epoch 14/100, Train Loss: 0.9871, Duration: 0:00:01.152140\n",
      "Epoch 15/100, Train Loss: 0.9752, Duration: 0:00:01.169379\n",
      "Epoch 16/100, Train Loss: 0.9714, Duration: 0:00:01.134976\n",
      "Epoch 17/100, Train Loss: 0.9714, Duration: 0:00:01.140230\n",
      "Epoch 18/100, Train Loss: 0.9558, Duration: 0:00:01.130702\n",
      "Epoch 19/100, Train Loss: 0.9474, Duration: 0:00:01.218951\n",
      "Epoch 20/100, Train Loss: 0.9460, Duration: 0:00:01.218957\n",
      "Epoch 21/100, Train Loss: 0.9498, Duration: 0:00:01.335893\n",
      "Epoch 22/100, Train Loss: 0.9375, Duration: 0:00:01.157014\n",
      "Epoch 23/100, Train Loss: 0.9304, Duration: 0:00:01.231244\n",
      "Epoch 24/100, Train Loss: 0.9226, Duration: 0:00:01.101576\n",
      "Epoch 25/100, Train Loss: 0.9162, Duration: 0:00:01.151987\n",
      "Epoch 26/100, Train Loss: 0.9101, Duration: 0:00:01.174828\n",
      "Epoch 27/100, Train Loss: 0.9045, Duration: 0:00:01.115841\n",
      "Epoch 28/100, Train Loss: 0.9093, Duration: 0:00:01.101631\n",
      "Epoch 29/100, Train Loss: 0.9068, Duration: 0:00:01.149690\n",
      "Epoch 30/100, Train Loss: 0.9065, Duration: 0:00:01.153300\n",
      "Epoch 31/100, Train Loss: 0.9006, Duration: 0:00:01.252403\n",
      "Epoch 32/100, Train Loss: 0.9008, Duration: 0:00:01.202191\n",
      "Epoch 33/100, Train Loss: 0.9203, Duration: 0:00:01.185513\n",
      "Epoch 34/100, Train Loss: 0.9143, Duration: 0:00:01.134149\n",
      "Epoch 35/100, Train Loss: 0.9235, Duration: 0:00:01.120008\n",
      "Epoch 36/100, Train Loss: 0.8921, Duration: 0:00:01.140831\n",
      "Epoch 37/100, Train Loss: 0.8904, Duration: 0:00:01.113513\n",
      "Epoch 38/100, Train Loss: 0.8910, Duration: 0:00:01.218804\n",
      "Epoch 39/100, Train Loss: 0.8863, Duration: 0:00:01.118920\n",
      "Epoch 40/100, Train Loss: 0.8790, Duration: 0:00:01.118763\n",
      "Epoch 41/100, Train Loss: 0.8741, Duration: 0:00:01.135317\n",
      "Epoch 42/100, Train Loss: 0.8710, Duration: 0:00:01.169058\n",
      "Epoch 43/100, Train Loss: 0.8743, Duration: 0:00:01.196754\n",
      "Epoch 44/100, Train Loss: 0.8627, Duration: 0:00:01.106546\n",
      "Epoch 45/100, Train Loss: 0.8576, Duration: 0:00:01.135476\n",
      "Epoch 46/100, Train Loss: 0.8502, Duration: 0:00:01.118763\n",
      "Epoch 47/100, Train Loss: 0.8521, Duration: 0:00:01.218978\n",
      "Epoch 48/100, Train Loss: 0.8486, Duration: 0:00:01.169620\n",
      "Epoch 49/100, Train Loss: 0.8497, Duration: 0:00:01.134693\n",
      "Epoch 50/100, Train Loss: 0.8566, Duration: 0:00:01.134000\n",
      "Epoch 51/100, Train Loss: 0.8624, Duration: 0:00:01.173084\n",
      "Epoch 52/100, Train Loss: 0.9052, Duration: 0:00:01.113871\n",
      "Epoch 53/100, Train Loss: 0.9153, Duration: 0:00:01.172532\n",
      "Epoch 54/100, Train Loss: 0.8979, Duration: 0:00:01.164051\n",
      "Epoch 55/100, Train Loss: 0.8777, Duration: 0:00:01.118763\n",
      "Epoch 56/100, Train Loss: 0.8839, Duration: 0:00:01.135479\n",
      "Epoch 57/100, Train Loss: 0.9133, Duration: 0:00:01.135478\n",
      "Epoch 58/100, Train Loss: 0.9199, Duration: 0:00:01.202256\n",
      "Epoch 59/100, Train Loss: 0.8804, Duration: 0:00:01.149351\n",
      "Epoch 60/100, Train Loss: 0.8631, Duration: 0:00:01.104907\n",
      "Epoch 61/100, Train Loss: 0.8513, Duration: 0:00:01.188790\n",
      "Epoch 62/100, Train Loss: 0.8430, Duration: 0:00:01.115557\n",
      "Epoch 63/100, Train Loss: 0.8392, Duration: 0:00:01.145319\n",
      "Epoch 64/100, Train Loss: 0.8357, Duration: 0:00:01.157749\n",
      "Epoch 65/100, Train Loss: 0.8342, Duration: 0:00:01.102041\n",
      "Epoch 66/100, Train Loss: 0.8397, Duration: 0:00:01.135598\n",
      "Epoch 67/100, Train Loss: 0.8382, Duration: 0:00:01.185426\n",
      "Epoch 68/100, Train Loss: 0.8385, Duration: 0:00:01.197726\n",
      "Epoch 69/100, Train Loss: 0.8356, Duration: 0:00:01.307007\n",
      "Epoch 70/100, Train Loss: 0.8479, Duration: 0:00:01.185643\n",
      "Epoch 71/100, Train Loss: 0.8499, Duration: 0:00:01.151947\n",
      "Epoch 72/100, Train Loss: 0.8433, Duration: 0:00:01.135565\n",
      "Epoch 73/100, Train Loss: 0.8473, Duration: 0:00:01.155211\n",
      "Epoch 74/100, Train Loss: 0.8470, Duration: 0:00:01.142220\n",
      "Epoch 75/100, Train Loss: 0.8356, Duration: 0:00:01.201355\n",
      "Epoch 76/100, Train Loss: 0.8369, Duration: 0:00:01.143325\n",
      "Epoch 77/100, Train Loss: 0.8343, Duration: 0:00:01.135510\n",
      "Epoch 78/100, Train Loss: 0.8315, Duration: 0:00:01.185459\n",
      "Epoch 79/100, Train Loss: 0.8294, Duration: 0:00:01.197430\n",
      "Epoch 80/100, Train Loss: 0.8305, Duration: 0:00:01.207683\n",
      "Epoch 81/100, Train Loss: 0.8280, Duration: 0:00:01.290246\n",
      "Epoch 82/100, Train Loss: 0.8257, Duration: 0:00:01.146189\n",
      "Epoch 83/100, Train Loss: 0.8359, Duration: 0:00:01.168847\n",
      "Epoch 84/100, Train Loss: 0.8378, Duration: 0:00:01.202083\n",
      "Epoch 85/100, Train Loss: 0.8426, Duration: 0:00:01.152332\n",
      "Epoch 86/100, Train Loss: 0.8698, Duration: 0:00:01.253871\n",
      "Epoch 87/100, Train Loss: 0.9633, Duration: 0:00:01.117859\n",
      "Epoch 88/100, Train Loss: 0.9077, Duration: 0:00:01.151400\n",
      "Epoch 89/100, Train Loss: 0.8748, Duration: 0:00:01.135515\n",
      "Epoch 90/100, Train Loss: 0.8471, Duration: 0:00:01.235650\n",
      "Epoch 91/100, Train Loss: 0.8331, Duration: 0:00:01.203446\n",
      "Epoch 92/100, Train Loss: 0.8245, Duration: 0:00:01.199706\n",
      "Epoch 93/100, Train Loss: 0.8185, Duration: 0:00:01.152675\n",
      "Epoch 94/100, Train Loss: 0.8156, Duration: 0:00:01.202239\n",
      "Epoch 95/100, Train Loss: 0.8171, Duration: 0:00:01.169345\n",
      "Epoch 96/100, Train Loss: 0.8180, Duration: 0:00:01.169378\n",
      "Epoch 97/100, Train Loss: 0.8151, Duration: 0:00:01.184706\n",
      "Epoch 98/100, Train Loss: 0.8131, Duration: 0:00:01.135266\n",
      "Epoch 99/100, Train Loss: 0.8117, Duration: 0:00:01.118702\n",
      "Epoch 100/100, Train Loss: 0.8111, Duration: 0:00:01.169007\n"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Romi\n",
      "<CLS> romi is a cat <SEP>\n",
      "Response: None\n"
     ]
    }
   ],
   "source": [
    "testInfer_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
