{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from Decoder import Decoder\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersCustom\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 238891\n"
     ]
    }
   ],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInferTokenIds(model, input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "    # logits will contain probabilities for each token    \n",
    "    print(\"Output shape:\", output.shape) # torch.Size([1, 12, 43])\n",
    "\n",
    "    # output contains the logits\n",
    "    # get the index for the highest logits for each token\n",
    "    predictionIdxs = torch.argmax(output, axis=-1)\n",
    "    print(\"Prediction Idxs shape:\", predictionIdxs.shape) # torch.Size([1, 12])\n",
    "    \n",
    "    # Convert to list\n",
    "    predictionIds = predictionIdxs.squeeze(0).tolist()\n",
    "    # Get token ids from idx\n",
    "    #predTokenIds = customTokenizer.getTokenIdsForIdxs(predictionIdxs)\n",
    "    return predictionIds\n",
    "     \n",
    "def getDecodedSentence(inputTokens):\n",
    "    return customTokenizer.decode(inputTokens)\n",
    "\n",
    "def runInference(model, prompt):\n",
    "    input = customTokenizer.encode(prompt) # will add start and end tokens\n",
    "    input = torch.tensor(input).unsqueeze(0).to(device)\n",
    "    predTokenIds = getInferTokenIds(model, input)\n",
    "    return getDecodedSentence(predTokenIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 43])\n",
      "Prediction Idxs shape: torch.Size([1, 3])\n",
      "the into <CLS> a\n"
     ]
    }
   ],
   "source": [
    "def testInfer_1():\n",
    "    # Check inference with current model\n",
    "    prompt = \"Romi\"\n",
    "    print(prompt + \" \" + runInference(model,prompt)) # All are lower case\n",
    "\n",
    "testInfer_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([154, 12])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "    rowsTrain = trainDataTensor.shape[0]\n",
    "    for i in range(rowsTrain):\n",
    "      #print(f\"{i}/{rowsTrain}\")\n",
    "      x_t = trainDataTensor[i].unsqueeze(0).to(device)\n",
    "     \n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "      # print(\"outputs:\", outputs)\n",
    "      # print(\"targets:\", targets)\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitanya Belwal\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set Optim and criterion\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId())\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.8444, Duration: 0:00:01.180187\n",
      "Epoch 2/100, Train Loss: 1.3306, Duration: 0:00:01.085437\n",
      "Epoch 3/100, Train Loss: 1.2296, Duration: 0:00:01.118766\n",
      "Epoch 4/100, Train Loss: 1.1878, Duration: 0:00:01.118737\n",
      "Epoch 5/100, Train Loss: 1.1404, Duration: 0:00:01.102614\n",
      "Epoch 6/100, Train Loss: 1.1192, Duration: 0:00:01.319247\n",
      "Epoch 7/100, Train Loss: 1.0601, Duration: 0:00:01.198073\n",
      "Epoch 8/100, Train Loss: 1.0706, Duration: 0:00:01.210647\n",
      "Epoch 9/100, Train Loss: 1.0346, Duration: 0:00:01.169813\n",
      "Epoch 10/100, Train Loss: 1.0246, Duration: 0:00:01.093041\n",
      "Epoch 11/100, Train Loss: 1.0284, Duration: 0:00:01.145280\n",
      "Epoch 12/100, Train Loss: 1.0486, Duration: 0:00:01.376723\n",
      "Epoch 13/100, Train Loss: 0.9973, Duration: 0:00:01.245384\n",
      "Epoch 14/100, Train Loss: 0.9959, Duration: 0:00:01.268329\n",
      "Epoch 15/100, Train Loss: 1.0042, Duration: 0:00:01.185550\n",
      "Epoch 16/100, Train Loss: 0.9806, Duration: 0:00:01.170249\n",
      "Epoch 17/100, Train Loss: 0.9733, Duration: 0:00:01.184194\n",
      "Epoch 18/100, Train Loss: 0.9675, Duration: 0:00:01.118668\n",
      "Epoch 19/100, Train Loss: 0.9454, Duration: 0:00:01.153650\n",
      "Epoch 20/100, Train Loss: 0.9441, Duration: 0:00:01.126658\n",
      "Epoch 21/100, Train Loss: 0.9388, Duration: 0:00:01.140106\n",
      "Epoch 22/100, Train Loss: 0.9340, Duration: 0:00:01.121444\n",
      "Epoch 23/100, Train Loss: 0.9346, Duration: 0:00:01.168364\n",
      "Epoch 24/100, Train Loss: 0.9224, Duration: 0:00:01.319171\n",
      "Epoch 25/100, Train Loss: 0.9129, Duration: 0:00:01.136701\n",
      "Epoch 26/100, Train Loss: 0.9130, Duration: 0:00:01.105369\n",
      "Epoch 27/100, Train Loss: 0.9217, Duration: 0:00:01.249604\n",
      "Epoch 28/100, Train Loss: 0.9131, Duration: 0:00:01.134373\n",
      "Epoch 29/100, Train Loss: 0.9122, Duration: 0:00:01.445367\n",
      "Epoch 30/100, Train Loss: 0.9072, Duration: 0:00:01.583564\n",
      "Epoch 31/100, Train Loss: 0.9020, Duration: 0:00:01.529218\n",
      "Epoch 32/100, Train Loss: 0.9190, Duration: 0:00:01.254746\n",
      "Epoch 33/100, Train Loss: 0.9163, Duration: 0:00:01.402754\n",
      "Epoch 34/100, Train Loss: 0.9281, Duration: 0:00:01.391182\n",
      "Epoch 35/100, Train Loss: 0.9212, Duration: 0:00:01.307245\n",
      "Epoch 36/100, Train Loss: 0.9100, Duration: 0:00:01.471480\n",
      "Epoch 37/100, Train Loss: 0.9177, Duration: 0:00:01.135473\n",
      "Epoch 38/100, Train Loss: 0.9058, Duration: 0:00:01.302641\n",
      "Epoch 39/100, Train Loss: 0.8938, Duration: 0:00:01.190485\n",
      "Epoch 40/100, Train Loss: 0.8873, Duration: 0:00:01.198399\n",
      "Epoch 41/100, Train Loss: 0.8817, Duration: 0:00:01.383463\n",
      "Epoch 42/100, Train Loss: 0.8662, Duration: 0:00:01.150092\n",
      "Epoch 43/100, Train Loss: 0.8684, Duration: 0:00:01.085913\n",
      "Epoch 44/100, Train Loss: 0.8673, Duration: 0:00:01.068648\n",
      "Epoch 45/100, Train Loss: 0.8665, Duration: 0:00:01.118624\n",
      "Epoch 46/100, Train Loss: 0.8694, Duration: 0:00:01.171661\n",
      "Epoch 47/100, Train Loss: 0.8621, Duration: 0:00:01.204640\n",
      "Epoch 48/100, Train Loss: 0.8593, Duration: 0:00:01.179565\n",
      "Epoch 49/100, Train Loss: 0.8547, Duration: 0:00:01.171241\n",
      "Epoch 50/100, Train Loss: 0.8578, Duration: 0:00:01.125089\n",
      "Epoch 51/100, Train Loss: 0.8575, Duration: 0:00:01.179752\n",
      "Epoch 52/100, Train Loss: 0.8623, Duration: 0:00:01.170824\n",
      "Epoch 53/100, Train Loss: 0.8754, Duration: 0:00:01.152213\n",
      "Epoch 54/100, Train Loss: 0.9035, Duration: 0:00:01.100825\n",
      "Epoch 55/100, Train Loss: 0.8849, Duration: 0:00:01.215800\n",
      "Epoch 56/100, Train Loss: 0.8742, Duration: 0:00:01.150620\n",
      "Epoch 57/100, Train Loss: 0.9118, Duration: 0:00:01.192460\n",
      "Epoch 58/100, Train Loss: 0.8950, Duration: 0:00:01.164601\n",
      "Epoch 59/100, Train Loss: 0.8678, Duration: 0:00:01.140621\n",
      "Epoch 60/100, Train Loss: 0.8567, Duration: 0:00:01.077586\n",
      "Epoch 61/100, Train Loss: 0.8512, Duration: 0:00:01.135438\n",
      "Epoch 62/100, Train Loss: 0.8423, Duration: 0:00:01.168889\n",
      "Epoch 63/100, Train Loss: 0.8346, Duration: 0:00:01.185515\n",
      "Epoch 64/100, Train Loss: 0.8325, Duration: 0:00:01.104906\n",
      "Epoch 65/100, Train Loss: 0.8326, Duration: 0:00:01.121907\n",
      "Epoch 66/100, Train Loss: 0.8316, Duration: 0:00:01.177767\n",
      "Epoch 67/100, Train Loss: 0.8402, Duration: 0:00:01.168852\n",
      "Epoch 68/100, Train Loss: 0.8523, Duration: 0:00:01.152121\n",
      "Epoch 69/100, Train Loss: 0.8386, Duration: 0:00:01.152064\n",
      "Epoch 70/100, Train Loss: 0.8369, Duration: 0:00:01.117674\n",
      "Epoch 71/100, Train Loss: 0.8303, Duration: 0:00:01.152662\n",
      "Epoch 72/100, Train Loss: 0.8374, Duration: 0:00:01.102004\n",
      "Epoch 73/100, Train Loss: 0.8324, Duration: 0:00:01.118801\n",
      "Epoch 74/100, Train Loss: 0.8336, Duration: 0:00:01.171340\n",
      "Epoch 75/100, Train Loss: 0.8521, Duration: 0:00:01.149840\n",
      "Epoch 76/100, Train Loss: 0.8917, Duration: 0:00:01.151655\n",
      "Epoch 77/100, Train Loss: 0.8832, Duration: 0:00:01.168711\n",
      "Epoch 78/100, Train Loss: 0.8780, Duration: 0:00:01.102052\n",
      "Epoch 79/100, Train Loss: 0.8573, Duration: 0:00:01.135372\n",
      "Epoch 80/100, Train Loss: 0.8414, Duration: 0:00:01.118734\n",
      "Epoch 81/100, Train Loss: 0.8339, Duration: 0:00:01.152310\n",
      "Epoch 82/100, Train Loss: 0.8244, Duration: 0:00:01.168752\n",
      "Epoch 83/100, Train Loss: 0.8208, Duration: 0:00:01.169121\n",
      "Epoch 84/100, Train Loss: 0.8176, Duration: 0:00:01.218347\n",
      "Epoch 85/100, Train Loss: 0.8164, Duration: 0:00:01.211641\n",
      "Epoch 86/100, Train Loss: 0.8147, Duration: 0:00:01.225785\n",
      "Epoch 87/100, Train Loss: 0.8135, Duration: 0:00:01.135342\n",
      "Epoch 88/100, Train Loss: 0.8134, Duration: 0:00:01.118885\n",
      "Epoch 89/100, Train Loss: 0.8149, Duration: 0:00:01.142965\n",
      "Epoch 90/100, Train Loss: 0.8147, Duration: 0:00:01.144627\n",
      "Epoch 91/100, Train Loss: 0.8133, Duration: 0:00:01.136237\n",
      "Epoch 92/100, Train Loss: 0.8163, Duration: 0:00:01.268382\n",
      "Epoch 93/100, Train Loss: 0.8322, Duration: 0:00:01.286297\n",
      "Epoch 94/100, Train Loss: 0.8346, Duration: 0:00:01.223970\n",
      "Epoch 95/100, Train Loss: 0.8399, Duration: 0:00:01.196597\n",
      "Epoch 96/100, Train Loss: 0.8931, Duration: 0:00:01.168854\n",
      "Epoch 97/100, Train Loss: 0.8838, Duration: 0:00:01.102088\n",
      "Epoch 98/100, Train Loss: 0.8712, Duration: 0:00:01.134947\n",
      "Epoch 99/100, Train Loss: 0.8528, Duration: 0:00:01.123951\n",
      "Epoch 100/100, Train Loss: 0.8329, Duration: 0:00:01.163699\n"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 12, 43])\n",
      "Prediction Idxs shape: torch.Size([1, 12])\n",
      "the the cat for always a a good <SEP> <SEP> good a a\n"
     ]
    }
   ],
   "source": [
    "testInfer_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
