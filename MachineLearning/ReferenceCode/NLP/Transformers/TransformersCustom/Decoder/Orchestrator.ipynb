{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from Decoder import Decoder\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersCustom\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getVocabSize()\n",
    "maxLen = customTokenizer.getMaxTokenId()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 105643\n"
     ]
    }
   ],
   "source": [
    "paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", paramCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInferTokenIds(model, input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "    # logits will contain probabilities for each token    \n",
    "    print(\"Output shape:\", output.shape) # torch.Size([1, 12, 43])\n",
    "\n",
    "    # output contains the logits\n",
    "    # get the index for the highest logit\n",
    "    predictionIdxs = torch.argmax(output, axis=-1)\n",
    "    print(\"Prediction Idxs shape:\", predictionIdxs.shape) # torch.Size([1, 12])\n",
    "    \n",
    "    # Convert to list\n",
    "    predictionIdxs = predictionIdxs.squeeze(0).tolist()\n",
    "    # Get token ids from idx\n",
    "    predTokenIds = customTokenizer.getTokenIdsForIdxs(predictionIdxs)\n",
    "    return predTokenIds\n",
    "     \n",
    "def getDecodedSentence(inputTokens):\n",
    "    return customTokenizer.decodeTokenizedSentence(inputTokens)\n",
    "\n",
    "def runInferenceTillEnd(model, startTokens):\n",
    "    input = customTokenizer.encodeTokenizedSentence(startTokens)\n",
    "    input = torch.tensor(input).unsqueeze(0).to(device)\n",
    "    #predTokenId = input[0][0]\n",
    "    \n",
    "    #while predTokenId != customTokenizer.getSepTokenId():\n",
    "    predTokenIds = getInferTokenIds(model, input)\n",
    "    #input = torch.cat((input, predTokenId.unsqueeze(0)), dim=-1)\n",
    "    #print(\"Output sentence:\", getDecodedSentence(predTokenIds))\n",
    "    return getDecodedSentence(predTokenIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 12, 43])\n",
      "Prediction Idxs shape: torch.Size([1, 12])\n",
      "Predicted Token Idx: 22\n",
      "Predicted Token Idx: 10\n",
      "Predicted Token Idx: 12\n",
      "Predicted Token Idx: 8\n",
      "Predicted Token Idx: 8\n",
      "Predicted Token Idx: 8\n",
      "Predicted Token Idx: 16\n",
      "Predicted Token Idx: 12\n",
      "Predicted Token Idx: 29\n",
      "Predicted Token Idx: 8\n",
      "Predicted Token Idx: 8\n",
      "Predicted Token Idx: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'to one for it it it at for large it it it'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check inference with current model\n",
    "runInferenceTillEnd(model,\"the\") # All are lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "    for i in range(trainDataTensor.shape[0]):\n",
    "      x_t = trainDataTensor[i].unsqueeze(0).to(device)\n",
    "      #for batch in train_loader:\n",
    "      # move data to GPU\n",
    "      #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "      # print(\"outputs:\", outputs)\n",
    "      # print(\"targets:\", targets)\n",
    "      loss = criterion(outputs.transpose(2, 1), targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optim and criterion\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId())\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections will deal with model inference and metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
