{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "import os,sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer\n",
    "from CCustomInference import CCustomInference\n",
    "from Decoder import Decoder\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from DifferentialPrivacy.CDP_SGD import CDP_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersDP\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Set seed for reproducibility across runs\n",
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 105643\n"
     ]
    }
   ],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embedding  Parameters: 2752\n",
      "Layer: pos_encoding Parameters: 0\n",
      "Layer: pos_encoding.dropout Parameters: 0\n",
      "Layer: transformer_blocks Parameters: 99968\n",
      "Layer: transformer_blocks.0 Parameters: 49984\n",
      "Layer: transformer_blocks.0.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.0.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.0.mha Parameters: 16640\n",
      "Layer: transformer_blocks.0.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.0.ann Parameters: 33088\n",
      "Layer: transformer_blocks.0.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.0.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.0.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.0.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.0.dropout Parameters: 0\n",
      "Layer: transformer_blocks.1 Parameters: 49984\n",
      "Layer: transformer_blocks.1.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.1.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.1.mha Parameters: 16640\n",
      "Layer: transformer_blocks.1.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.1.ann Parameters: 33088\n",
      "Layer: transformer_blocks.1.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.1.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.1.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.1.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.1.dropout Parameters: 0\n",
      "Layer: ln         Parameters: 128\n",
      "Layer: fc         Parameters: 2795\n"
     ]
    }
   ],
   "source": [
    "# Parameters for each layer in cascading format\n",
    "# Embeddings layer, input: 43, output: 64.\n",
    "# Total params: 43*64 = 2752, embeddings do not have bias.\n",
    "for name, layer in model.named_modules():\n",
    "    if name != \"\":\n",
    "        total_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "        print(f\"Layer: {name:<10} Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Infer(prompt, temperature=0.0, topP=1.0):\n",
    "    infer = CCustomInference(model, customTokenizer, device, debug=False)\n",
    "    print(f\"{infer.getInferenceOutput(prompt, temperature=temperature,topP=topP)}\") # All are lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> romi romi romi romi romi romi romi romi romi romi romi\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "def testInfer_1(temperature=0.0):\n",
    "    prompt = \"Romi\"\n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_2(temperature=0.0):\n",
    "    prompt = \"\" \n",
    "    Infer(prompt, temperature)\n",
    "    \n",
    "# Check inference with current model\n",
    "testInfer_1(0)\n",
    "#testInfer_2(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([154, 12])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "useDPSGD = True\n",
    "#------ DP_SGD Parameters\n",
    "eps = 1\n",
    "delta = 1e-7 #.5\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  # create object for DP-SGD here so that it can be reint in each call.\n",
    "  countRowsTrain = trainDataTensor.shape[0]\n",
    "  dpsgd = CDP_SGD(model=model, learningRate=learningRate,delta=delta,eps=eps,totalSamples=countRowsTrain) # Use defaults  \n",
    "  for iter in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "\n",
    "    idxTrain = 0\n",
    "    for i in range(countRowsTrain): \n",
    "      # For DP-SGD the values have to be picked with sampling probability L/N \n",
    "      # In our case L = 1 \n",
    "      if useDPSGD: # pick random sample\n",
    "        # Pick a random sample from the training data\n",
    "        idxTrain = np.random.randint(0, countRowsTrain)\n",
    "      else:\n",
    "        idxTrain = i # Go through all samples sequentially in the training data\n",
    "      x_t = trainDataTensor[idxTrain].unsqueeze(0).to(device)\n",
    "     \n",
    "      #if not useDPSGD:\n",
    "      idxTrain = i # Go through all samples sequentially in the training data\n",
    "      optimizer.zero_grad() # set all grads to 0\n",
    "        \n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      # This will update the grad values in the model parameters\n",
    "      loss.backward()\n",
    "\n",
    "      # Apply DP-SGD here\n",
    "      if useDPSGD:\n",
    "        dpsgd.singleStep()\n",
    "      else:\n",
    "        optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[iter] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {iter+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "    if useDPSGD:\n",
    "        (spentEps, spentDelta) = dpsgd.getPrivacySpent()\n",
    "        print(f\"*** Using DPSGD, Spent: eps={spentEps}, delta={spentDelta}  ***\")\n",
    "        if dpsgd.hasReachedPrivacyLimit(): # Check at tend of each epoch\n",
    "          print(\"Privacy budget used up, stopping training\")\n",
    "          break\n",
    "\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# CrossEntropyLoss is meant for classification problems\n",
    "# \n",
    "# from: https://developers.google.com/machine-learning/glossary/#logits\n",
    "# logits:The vector of raw (non-normalized) predictions that a classification model generates, \n",
    "# which is ordinarily then passed to a normalization function\n",
    "# \n",
    "# input: The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general)\n",
    "# hence the input will be a vector\n",
    "#\n",
    "# target: is Class indices in the range [0,C)where C is the number of classes; \n",
    "# if ignore_index is specified, this loss also accepts this class index \n",
    "# (this index may not necessarily be in the class range).\n",
    "# Here the number of classes is the vocab size\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId()) \n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 3.7535, Duration: 0:00:01.054882\n",
      "*** Using DPSGD, Spent: eps=6.154345989227295, delta=6.493506288229867e-10  ***\n",
      "Epoch 2/10, Train Loss: 3.7541, Duration: 0:00:01.013905\n",
      "*** Using DPSGD, Spent: eps=8.703575134277344, delta=6.493506288229867e-10  ***\n",
      "Epoch 3/10, Train Loss: 3.7547, Duration: 0:00:01.001907\n",
      "*** Using DPSGD, Spent: eps=10.658942222595215, delta=6.493506288229867e-10  ***\n",
      "Epoch 4/10, Train Loss: 3.7545, Duration: 0:00:00.987695\n",
      "*** Using DPSGD, Spent: eps=12.307473182678223, delta=6.493506288229867e-10  ***\n",
      "Epoch 5/10, Train Loss: 3.7544, Duration: 0:00:00.988114\n",
      "*** Using DPSGD, Spent: eps=13.759895324707031, delta=6.493506288229867e-10  ***\n",
      "Epoch 6/10, Train Loss: 3.7543, Duration: 0:00:00.982139\n",
      "*** Using DPSGD, Spent: eps=15.073007583618164, delta=6.493506288229867e-10  ***\n",
      "Epoch 7/10, Train Loss: 3.7543, Duration: 0:00:01.004642\n",
      "*** Using DPSGD, Spent: eps=16.280553817749023, delta=6.493506288229867e-10  ***\n",
      "Epoch 8/10, Train Loss: 3.7545, Duration: 0:00:00.981936\n",
      "*** Using DPSGD, Spent: eps=17.404518127441406, delta=6.493506288229867e-10  ***\n",
      "Epoch 9/10, Train Loss: 3.7540, Duration: 0:00:00.966992\n",
      "*** Using DPSGD, Spent: eps=18.460176467895508, delta=6.493506288229867e-10  ***\n",
      "Epoch 10/10, Train Loss: 3.7548, Duration: 0:00:01.040794\n",
      "*** Using DPSGD, Spent: eps=19.458646774291992, delta=6.493506288229867e-10  ***\n"
     ]
    }
   ],
   "source": [
    "#print(list(model.parameters()))\n",
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> romi romi romi romi romi romi romi romi romi romi romi\n"
     ]
    }
   ],
   "source": [
    "testInfer_1(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
