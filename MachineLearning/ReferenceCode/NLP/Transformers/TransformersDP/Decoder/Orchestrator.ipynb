{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from Decoder import Decoder\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer\n",
    "from CCustomInference import CCustomInference\n",
    "from datetime import datetime\n",
    "from CDP_SGD import CDP_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersDP\\Decoder\n",
      "Number of tokens: 43\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimension\n",
    "dimEmbeddings = 64 # 64 embeddinds\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Set seed for reproducibility across runs\n",
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 105643\n"
     ]
    }
   ],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embedding  Parameters: 2752\n",
      "Layer: pos_encoding Parameters: 0\n",
      "Layer: pos_encoding.dropout Parameters: 0\n",
      "Layer: transformer_blocks Parameters: 99968\n",
      "Layer: transformer_blocks.0 Parameters: 49984\n",
      "Layer: transformer_blocks.0.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.0.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.0.mha Parameters: 16640\n",
      "Layer: transformer_blocks.0.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.0.ann Parameters: 33088\n",
      "Layer: transformer_blocks.0.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.0.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.0.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.0.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.0.dropout Parameters: 0\n",
      "Layer: transformer_blocks.1 Parameters: 49984\n",
      "Layer: transformer_blocks.1.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.1.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.1.mha Parameters: 16640\n",
      "Layer: transformer_blocks.1.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.1.ann Parameters: 33088\n",
      "Layer: transformer_blocks.1.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.1.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.1.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.1.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.1.dropout Parameters: 0\n",
      "Layer: ln         Parameters: 128\n",
      "Layer: fc         Parameters: 2795\n"
     ]
    }
   ],
   "source": [
    "# Parameters for each layer in cascading format\n",
    "# Embeddings layer, input: 43, output: 64.\n",
    "# Total params: 43*64 = 2752, embeddings do not have bias.\n",
    "for name, layer in model.named_modules():\n",
    "    if name != \"\":\n",
    "        total_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "        print(f\"Layer: {name:<10} Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(43, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Infer(prompt, temperature=0.0, topP=1.0):\n",
    "    infer = CCustomInference(model, customTokenizer, device, debug=False)\n",
    "    print(f\"{infer.getInferenceOutput(prompt, temperature=temperature,topP=topP)}\") # All are lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> cat house jumps cat house place cat place black over\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "def testInfer_1(temperature=0.0):\n",
    "    prompt = \"Romi\"\n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_2(temperature=0.0):\n",
    "    prompt = \"\" \n",
    "    Infer(prompt, temperature)\n",
    "    \n",
    "# Check inference with current model\n",
    "#testInfer_1(0)\n",
    "testInfer_2(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([154, 12])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.01\n",
    "useDPSGD = True\n",
    "dpsgd = CDP_SGD(model, learningRate)\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  \n",
    "  for iter in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "    countRowsTrain = trainDataTensor.shape[0]\n",
    "    for i in range(countRowsTrain):\n",
    "      #print(f\"{i}/{rowsTrain}\")\n",
    "      x_t = trainDataTensor[i].unsqueeze(0).to(device)\n",
    "     \n",
    "      # zero the parameter gradients\n",
    "      if not useDPSGD:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "      #print(\"input:\", x_t)\n",
    "      #print(\"outputs:\", outputs)\n",
    "      #print(\"targets:\", targets)\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      # This will update the grad values in the model parameters\n",
    "      loss.backward()\n",
    "\n",
    "      # Apply DP-SGD here\n",
    "      if useDPSGD:\n",
    "        dpsgd.singleStep()\n",
    "      else:\n",
    "        optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[iter] = train_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {iter+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# CrossEntropyLoss is meant for classification problems\n",
    "# \n",
    "# from: https://developers.google.com/machine-learning/glossary/#logits\n",
    "# logits:The vector of raw (non-normalized) predictions that a classification model generates, \n",
    "# which is ordinarily then passed to a normalization function\n",
    "# \n",
    "# input: The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general)\n",
    "# hence the input will be a vector\n",
    "#\n",
    "# target: is Class indices in the range [0,C)where C is the number of classes; \n",
    "# if ignore_index is specified, this loss also accepts this class index \n",
    "# (this index may not necessarily be in the class range).\n",
    "# Here the number of classes is the vocab size\n",
    "#\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId()) \n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "assigned grad expected to be a Tensor or None but got grad of type int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#print(list(model.parameters()))\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Start the training loop\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply DP-SGD here\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m useDPSGD:\n\u001b[1;32m---> 55\u001b[0m   \u001b[43mdpsgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingleStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update the parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersDP\\Decoder\\CDP_SGD.py:50\u001b[0m, in \u001b[0;36mCDP_SGD.singleStep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m     param \u001b[38;5;241m=\u001b[39m param \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m sanitizedGrads\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Note: In the Medeium article, the noise is added after gradient is computed\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# which is at this point. But in Abadi et al. the noise is added before\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# Reset for next iteration\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#self.computePrivacyLoss(param.grad, param.data)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: assigned grad expected to be a Tensor or None but got grad of type int"
     ]
    }
   ],
   "source": [
    "#print(list(model.parameters()))\n",
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> romi is a cat <SEP>\n"
     ]
    }
   ],
   "source": [
    "testInfer_1(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
