{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TODO: figure our why manual update ot gradient ar enot working.\n",
    "\n",
    "import torch\n",
    "import os,sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer\n",
    "from CCustomInference import CCustomInference\n",
    "from Decoder import Decoder\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from DifferentialPrivacy.CDP_SGD import CDP_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions for the Transformer model\n",
    "dimEmbeddings = 64 # 64 embeddings\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Set seed for reproducibility across runs\n",
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for each layer in cascading format\n",
    "# Embeddings layer, input: 43, output: 64.\n",
    "# Total params: 43*64 = 2752, embeddings do not have bias.\n",
    "for name, layer in model.named_modules():\n",
    "    if name != \"\":\n",
    "        total_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "        print(f\"Layer: {name:<10} Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Infer(prompt, temperature=0.0, topP=1.0):\n",
    "    infer = CCustomInference(model, customTokenizer, device, debug=False)\n",
    "    print(f\"{infer.getInferenceOutput(prompt, temperature=temperature,topP=topP)}\") # All are lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "def testInfer_1(temperature=0.0):\n",
    "    prompt = \"Romi\"\n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_2(temperature=0.0):\n",
    "    prompt = \"Romi is a\" \n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_3(temperature=0.0):\n",
    "    prompt = \"\" \n",
    "    Infer(prompt, temperature)\n",
    "    \n",
    "# Check inference with current model\n",
    "testInfer_1(0)\n",
    "#testInfer_2(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useDPSGD = True\n",
    "bestModel = None\n",
    "if useDPSGD:\n",
    "  learningRate = 0.001 # Can implement adaptive learning rate\n",
    "  noOfEpochs = 30\n",
    "else:\n",
    "  learningRate = 0.01\n",
    "  noOfEpochs = 10\n",
    "#------ DP_SGD Parameters\n",
    "eps = .1\n",
    "delta = 1e-7 #.5\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  lowestLoss = sys.float_info.max\n",
    "  # create object for DP-SGD here so that it can be reint in each call.\n",
    "  countRowsTrain = trainDataTensor.shape[0]\n",
    "  dpsgd = CDP_SGD(model=model, learningRate=learningRate,delta=delta,eps=eps,totalSamples=countRowsTrain) # Use defaults  \n",
    "  \n",
    "  for iter in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "\n",
    "    idxTrain = 0\n",
    "    for i in range(countRowsTrain): \n",
    "      # For DP-SGD the values have to be picked with sampling probability L/N \n",
    "      # In our case L = 1 \n",
    "      if useDPSGD: # pick random sample\n",
    "        # Pick a random sample from the training data\n",
    "        idxTrain = np.random.randint(0, countRowsTrain)\n",
    "      else:\n",
    "        idxTrain = i # Go through all samples sequentially in the training data\n",
    "      idxTrain = i # Temporary\n",
    "      x_t = trainDataTensor[idxTrain].unsqueeze(0).to(device)\n",
    "     \n",
    "      if not useDPSGD:\n",
    "        optimizer.zero_grad() # set all grads to 0\n",
    "      else:\n",
    "        dpsgd.setZeroGrads()\n",
    "        \n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      # This will update the grad values in the model parameters\n",
    "      loss.backward()\n",
    "      \n",
    "      # Apply DP-SGD here\n",
    "      if useDPSGD:\n",
    "        dpsgd.singleStep()\n",
    "      else:\n",
    "        optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[iter] = train_loss\n",
    "    \n",
    "    if train_loss < lowestLoss:\n",
    "      bestModel = copy.deepcopy(model)\n",
    "      lowestLoss = train_loss\n",
    "      \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {iter+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "    if useDPSGD:\n",
    "        (spentEps, spentDelta) = dpsgd.getPrivacySpent()\n",
    "        print(f\"*** Using DPSGD, Spent: ε={spentEps}, δ={spentDelta}  ***\")\n",
    "        if dpsgd.hasReachedPrivacyLimit(): # Check at tend of each epoch\n",
    "          print(\"Privacy budget used up, stopping training\")\n",
    "          break\n",
    "\n",
    "  # Set model to best model\n",
    "  if bestModel is not None:\n",
    "    print(f\"### Best model has loss: {lowestLoss:.4f}\")\n",
    "    model = copy.deepcopy(bestModel)\n",
    "  \n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# CrossEntropyLoss is meant for classification problems\n",
    "# \n",
    "# from: https://developers.google.com/machine-learning/glossary/#logits\n",
    "# logits:The vector of raw (non-normalized) predictions that a classification model generates, \n",
    "# which is ordinarily then passed to a normalization function\n",
    "# \n",
    "# input: The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general)\n",
    "# hence the input will be a vector\n",
    "#\n",
    "# target: is Class indices in the range [0,C)where C is the number of classes; \n",
    "# if ignore_index is specified, this loss also accepts this class index \n",
    "# (this index may not necessarily be in the class range).\n",
    "# Here the number of classes is the vocab size\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId()) \n",
    "# Set the optimizer\n",
    "# Use SGG to be consistent with the manual methods used with DPSGD=true\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=learningRate)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=noOfEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInfer_2(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
