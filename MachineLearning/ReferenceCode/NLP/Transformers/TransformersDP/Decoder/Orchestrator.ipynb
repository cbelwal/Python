{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TODO: figure our why manual update ot gradient ar enot working.\n",
    "\n",
    "import torch\n",
    "import os,sys\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from CCustomTokenizer import CCustomTokenizer\n",
    "from CCustomInference import CCustomInference\n",
    "from Decoder import Decoder\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from DifferentialPrivacy.CDP_SGD import CDP_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA assertions\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChaitanyaBelwal\\ACTIVE\\Development\\GitHub\\Python\\MachineLearning\\ReferenceCode\\NLP\\Transformers\\TransformersDP\\Decoder\n",
      "Number of tokens: 46\n"
     ]
    }
   ],
   "source": [
    "# print current path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the tokenizer\n",
    "customTokenizer = CCustomTokenizer(\"../../data/SampleSentencesCorrected.txt\")\n",
    "print(\"Number of tokens:\",customTokenizer.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions for the Transformer model\n",
    "dimEmbeddings = 64 # 64 embeddings\n",
    "VocabSize = customTokenizer.getMaxTokenId() # Since the embedding layer is index based used the idx\n",
    "maxLen = customTokenizer.getMaxLen()\n",
    "attentionKeysSize = 16 # size of q,k and v. Attention output size = noOfHeads*attentionKeysSize\n",
    "noOfHeads = 4\n",
    "noOfTransformerBlocks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Decoder, set the specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # Set seed for reproducibility across runs\n",
    "model = Decoder(vocab_size = VocabSize,\n",
    "                 max_len= maxLen, \n",
    "                 d_k = attentionKeysSize, \n",
    "                 d_model = dimEmbeddings, \n",
    "                 n_heads = noOfHeads, \n",
    "                 n_layers = noOfTransformerBlocks,\n",
    "                 dropout_prob = 0.0) # 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable model parameters: 106030\n"
     ]
    }
   ],
   "source": [
    "#paramCount = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# Trainable model parameters:\", model.getParamCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embedding  Parameters: 2944\n",
      "Layer: pos_encoding Parameters: 0\n",
      "Layer: pos_encoding.dropout Parameters: 0\n",
      "Layer: transformer_blocks Parameters: 99968\n",
      "Layer: transformer_blocks.0 Parameters: 49984\n",
      "Layer: transformer_blocks.0.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.0.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.0.mha Parameters: 16640\n",
      "Layer: transformer_blocks.0.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.0.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.0.ann Parameters: 33088\n",
      "Layer: transformer_blocks.0.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.0.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.0.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.0.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.0.dropout Parameters: 0\n",
      "Layer: transformer_blocks.1 Parameters: 49984\n",
      "Layer: transformer_blocks.1.ln1 Parameters: 128\n",
      "Layer: transformer_blocks.1.ln2 Parameters: 128\n",
      "Layer: transformer_blocks.1.mha Parameters: 16640\n",
      "Layer: transformer_blocks.1.mha.key Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.query Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.value Parameters: 4160\n",
      "Layer: transformer_blocks.1.mha.fc Parameters: 4160\n",
      "Layer: transformer_blocks.1.ann Parameters: 33088\n",
      "Layer: transformer_blocks.1.ann.0 Parameters: 16640\n",
      "Layer: transformer_blocks.1.ann.1 Parameters: 0\n",
      "Layer: transformer_blocks.1.ann.2 Parameters: 16448\n",
      "Layer: transformer_blocks.1.ann.3 Parameters: 0\n",
      "Layer: transformer_blocks.1.dropout Parameters: 0\n",
      "Layer: ln         Parameters: 128\n",
      "Layer: fc         Parameters: 2990\n"
     ]
    }
   ],
   "source": [
    "# Parameters for each layer in cascading format\n",
    "# Embeddings layer, input: 43, output: 64.\n",
    "# Total params: 43*64 = 2752, embeddings do not have bias.\n",
    "for name, layer in model.named_modules():\n",
    "    if name != \"\":\n",
    "        total_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "        print(f\"Layer: {name:<10} Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(46, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"CUDA:\",torch.cuda.is_available())\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Infer(prompt, temperature=0.0, topP=1.0):\n",
    "    infer = CCustomInference(model, customTokenizer, device, debug=False)\n",
    "    print(f\"{infer.getInferenceOutput(prompt, temperature=temperature,topP=topP)}\") # All are lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> romi is a for can can can can likes romi likes\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "def testInfer_1(temperature=0.0):\n",
    "    prompt = \"Romi\"\n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_2(temperature=0.0):\n",
    "    prompt = \"Romi is a\" \n",
    "    Infer(prompt, temperature)\n",
    "\n",
    "def testInfer_3(temperature=0.0):\n",
    "    prompt = \"\" \n",
    "    Infer(prompt, temperature)\n",
    "    \n",
    "# Check inference with current model\n",
    "testInfer_2(0)\n",
    "#testInfer_2(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([156, 12])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "trainData = customTokenizer.getAllTrainingRows()\n",
    "trainDataTensor = torch.tensor(trainData)\n",
    "print(\"Train data shape:\", trainDataTensor.shape)\n",
    "# Shape is [154, 12]: 154 samples with 12 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useDPSGD = True\n",
    "bestModel = None\n",
    "if useDPSGD:\n",
    "  learningRate = 0.001 # Can implement adaptive learning rate\n",
    "  noOfEpochs = 30\n",
    "else:\n",
    "  learningRate = 0.01\n",
    "  noOfEpochs = 10\n",
    "#------ DP_SGD Parameters\n",
    "eps = .1\n",
    "delta = 1e-7 #.5\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "# N - batch size \n",
    "# T - sequence length (number of tokens in a sentence)\n",
    "# V - vocab size\n",
    "def train(model, criterion, optimizer, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  lowestLoss = sys.float_info.max\n",
    "  # create object for DP-SGD here so that it can be reint in each call.\n",
    "  countRowsTrain = trainDataTensor.shape[0]\n",
    "  dpsgd = CDP_SGD(model=model, learningRate=learningRate,delta=delta,eps=eps,totalSamples=countRowsTrain) # Use defaults  \n",
    "  \n",
    "  for iter in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    # Go through each sample in the training dataset\n",
    "    # update the model parameters after each sample like SGD\n",
    "    # each row of trainingDataTensor\n",
    "\n",
    "    idxTrain = 0\n",
    "    for i in range(countRowsTrain): \n",
    "      # For DP-SGD the values have to be picked with sampling probability L/N \n",
    "      # In our case L = 1 \n",
    "      if useDPSGD: # pick random sample\n",
    "        # Pick a random sample from the training data\n",
    "        idxTrain = np.random.randint(0, countRowsTrain)\n",
    "      else:\n",
    "        idxTrain = i # Go through all samples sequentially in the training data\n",
    "      idxTrain = i # Temporary\n",
    "      x_t = trainDataTensor[idxTrain].unsqueeze(0).to(device)\n",
    "     \n",
    "      if not useDPSGD:\n",
    "        optimizer.zero_grad() # set all grads to 0\n",
    "      else:\n",
    "        dpsgd.setZeroGrads()\n",
    "        \n",
    "      # shift targets backwards\n",
    "      # Original: <CLS> The cat sat on the mat <SEP>\n",
    "      # Becomes: The cat sat on the mat <SEP> <PAD>\n",
    "      targets = x_t.clone().detach()\n",
    "      # shifts = -1, will shift the target to left by 1\n",
    "      targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "      # PAD token is ignored in the loss so set last token to PAD\n",
    "      targets[:, -1] = customTokenizer.getPadTokenId()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(x_t)\n",
    "      # outputs are N x T x V\n",
    "      # but PyTorch expects N x V x T\n",
    "\n",
    "      transposedOutputs = outputs.transpose(2, 1)\n",
    "      loss = criterion(transposedOutputs, targets)\n",
    "      # N, T, V = outputs.shape\n",
    "      # loss = criterion(outputs.view(N * T, V), targets.view(N * T))\n",
    "        \n",
    "      # Backward and optimize\n",
    "      # This will update the grad values in the model parameters\n",
    "      loss.backward()\n",
    "      \n",
    "      # Apply DP-SGD here\n",
    "      if useDPSGD:\n",
    "        dpsgd.singleStep()\n",
    "      else:\n",
    "        optimizer.step() # update the parameters\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[iter] = train_loss\n",
    "    \n",
    "    if train_loss < lowestLoss:\n",
    "      bestModel = copy.deepcopy(model)\n",
    "      lowestLoss = train_loss\n",
    "      \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {iter+1}/{epochs}, Train Loss: {train_loss:.4f}, Duration: {dt}')\n",
    "    if useDPSGD:\n",
    "        (spentEps, spentDelta) = dpsgd.getPrivacySpent()\n",
    "        print(f\"*** Using DPSGD, Spent: ε={spentEps}, δ={spentDelta}  ***\")\n",
    "        if dpsgd.hasReachedPrivacyLimit(): # Check at tend of each epoch\n",
    "          print(\"Privacy budget used up, stopping training\")\n",
    "          break\n",
    "\n",
    "  # Set model to best model\n",
    "  if bestModel is not None:\n",
    "    print(f\"### Best model has loss: {lowestLoss:.4f}\")\n",
    "    model = copy.deepcopy(bestModel)\n",
    "  \n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitanya Belwal\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "# CrossEntropyLoss is meant for classification problems\n",
    "# \n",
    "# from: https://developers.google.com/machine-learning/glossary/#logits\n",
    "# logits:The vector of raw (non-normalized) predictions that a classification model generates, \n",
    "# which is ordinarily then passed to a normalization function\n",
    "# \n",
    "# input: The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general)\n",
    "# hence the input will be a vector\n",
    "#\n",
    "# target: is Class indices in the range [0,C)where C is the number of classes; \n",
    "# if ignore_index is specified, this loss also accepts this class index \n",
    "# (this index may not necessarily be in the class range).\n",
    "# Here the number of classes is the vocab size\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= customTokenizer.getPadTokenId()) \n",
    "# Set the optimizer\n",
    "# Use SGG to be consistent with the manual methods used with DPSGD=true\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=learningRate)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 3.3610, Duration: 0:00:01.136649\n",
      "*** Using DPSGD, Spent: eps=3.9280078411102295, delta=6.410256769839862e-10  ***\n",
      "Epoch 2/30, Train Loss: 3.2607, Duration: 0:00:01.320642\n",
      "*** Using DPSGD, Spent: eps=5.555427074432373, delta=6.410256769839862e-10  ***\n",
      "Epoch 3/30, Train Loss: 3.0150, Duration: 0:00:01.133373\n",
      "*** Using DPSGD, Spent: eps=6.8033833503723145, delta=6.410256769839862e-10  ***\n",
      "Epoch 4/30, Train Loss: 3.0638, Duration: 0:00:01.103302\n",
      "*** Using DPSGD, Spent: eps=7.855472564697266, delta=6.410256769839862e-10  ***\n",
      "Epoch 5/30, Train Loss: 2.8527, Duration: 0:00:01.184778\n",
      "*** Using DPSGD, Spent: eps=8.782418251037598, delta=6.410256769839862e-10  ***\n",
      "Epoch 6/30, Train Loss: 2.7016, Duration: 0:00:01.255205\n",
      "*** Using DPSGD, Spent: eps=9.620462417602539, delta=6.410256769839862e-10  ***\n",
      "Epoch 7/30, Train Loss: 2.6426, Duration: 0:00:01.132765\n",
      "*** Using DPSGD, Spent: eps=10.39113712310791, delta=6.410256769839862e-10  ***\n",
      "Epoch 8/30, Train Loss: 2.9918, Duration: 0:00:01.129788\n",
      "*** Using DPSGD, Spent: eps=11.108471870422363, delta=6.410256769839862e-10  ***\n",
      "Epoch 9/30, Train Loss: 2.7269, Duration: 0:00:01.141419\n",
      "*** Using DPSGD, Spent: eps=11.783560752868652, delta=6.410256769839862e-10  ***\n",
      "Epoch 10/30, Train Loss: 2.7035, Duration: 0:00:01.134009\n",
      "*** Using DPSGD, Spent: eps=12.422558784484863, delta=6.410256769839862e-10  ***\n",
      "Epoch 11/30, Train Loss: 2.6513, Duration: 0:00:01.082886\n",
      "*** Using DPSGD, Spent: eps=13.030257225036621, delta=6.410256769839862e-10  ***\n",
      "Epoch 12/30, Train Loss: 2.6634, Duration: 0:00:01.121355\n",
      "*** Using DPSGD, Spent: eps=13.610851287841797, delta=6.410256769839862e-10  ***\n",
      "Epoch 13/30, Train Loss: 2.6674, Duration: 0:00:01.137852\n",
      "*** Using DPSGD, Spent: eps=14.167671203613281, delta=6.410256769839862e-10  ***\n",
      "Epoch 14/30, Train Loss: 2.6422, Duration: 0:00:01.166149\n",
      "*** Using DPSGD, Spent: eps=14.703420639038086, delta=6.410256769839862e-10  ***\n",
      "Epoch 15/30, Train Loss: 2.6570, Duration: 0:00:01.252285\n",
      "*** Using DPSGD, Spent: eps=15.220322608947754, delta=6.410256769839862e-10  ***\n",
      "Epoch 16/30, Train Loss: 2.6542, Duration: 0:00:01.119101\n",
      "*** Using DPSGD, Spent: eps=15.720237731933594, delta=6.410256769839862e-10  ***\n",
      "Epoch 17/30, Train Loss: 2.6806, Duration: 0:00:01.224085\n",
      "*** Using DPSGD, Spent: eps=16.203548431396484, delta=6.410256769839862e-10  ***\n",
      "Epoch 18/30, Train Loss: 2.6817, Duration: 0:00:01.178096\n",
      "*** Using DPSGD, Spent: eps=16.671297073364258, delta=6.410256769839862e-10  ***\n",
      "Epoch 19/30, Train Loss: 2.6736, Duration: 0:00:01.217052\n",
      "*** Using DPSGD, Spent: eps=17.12627601623535, delta=6.410256769839862e-10  ***\n",
      "Epoch 20/30, Train Loss: 2.6582, Duration: 0:00:01.135484\n",
      "*** Using DPSGD, Spent: eps=17.569477081298828, delta=6.410256769839862e-10  ***\n",
      "Epoch 21/30, Train Loss: 2.6758, Duration: 0:00:01.186709\n",
      "*** Using DPSGD, Spent: eps=18.00177001953125, delta=6.410256769839862e-10  ***\n",
      "Epoch 22/30, Train Loss: 2.6785, Duration: 0:00:01.135512\n",
      "*** Using DPSGD, Spent: eps=18.42392349243164, delta=6.410256769839862e-10  ***\n",
      "Epoch 23/30, Train Loss: 2.7328, Duration: 0:00:01.151522\n",
      "*** Using DPSGD, Spent: eps=18.83661651611328, delta=6.410256769839862e-10  ***\n",
      "Epoch 24/30, Train Loss: 2.7601, Duration: 0:00:01.124445\n",
      "*** Using DPSGD, Spent: eps=19.240461349487305, delta=6.410256769839862e-10  ***\n",
      "Epoch 25/30, Train Loss: 2.7664, Duration: 0:00:01.322716\n",
      "*** Using DPSGD, Spent: eps=19.636001586914062, delta=6.410256769839862e-10  ***\n",
      "Epoch 26/30, Train Loss: 2.8717, Duration: 0:00:01.120245\n",
      "*** Using DPSGD, Spent: eps=20.02372932434082, delta=6.410256769839862e-10  ***\n",
      "Epoch 27/30, Train Loss: 2.9927, Duration: 0:00:01.141067\n",
      "*** Using DPSGD, Spent: eps=20.404090881347656, delta=6.410256769839862e-10  ***\n",
      "Epoch 28/30, Train Loss: 2.8864, Duration: 0:00:01.118734\n",
      "*** Using DPSGD, Spent: eps=20.777490615844727, delta=6.410256769839862e-10  ***\n",
      "Epoch 29/30, Train Loss: 2.8659, Duration: 0:00:01.118680\n",
      "*** Using DPSGD, Spent: eps=21.144298553466797, delta=6.410256769839862e-10  ***\n",
      "Epoch 30/30, Train Loss: 2.8395, Duration: 0:00:01.134475\n",
      "*** Using DPSGD, Spent: eps=21.50484848022461, delta=6.410256769839862e-10  ***\n",
      "### Best model has loss: 2.6422\n"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "train_losses = train(\n",
    "    model, criterion, optimizer, epochs=noOfEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been trained, following sections will deal with model inference and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> romi is a <SEP>\n"
     ]
    }
   ],
   "source": [
    "testInfer_2(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
