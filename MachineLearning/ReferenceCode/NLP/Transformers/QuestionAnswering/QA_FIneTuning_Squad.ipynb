{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the SQuAD (Standford Question and Answer Dataset) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "#raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some value of the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\",raw_datasets)\n",
    "\n",
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation DS can have multiple answers\n",
    "raw_datasets[\"validation\"][2][\"answers\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with sample data\n",
    "context = raw_datasets[\"train\"][1][\"context\"]\n",
    "question = raw_datasets[\"train\"][1][\"question\"]\n",
    "\n",
    "# Note: Inputs only contain a single row of data\n",
    "inputs = tokenizer(question, context)\n",
    "print(\"Raw token ids:\",inputs)\n",
    "\n",
    "# Decode the token ids\n",
    "print(\"Decoded tokens:\",tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex tokenizer\n",
    "# Split context into multiple chunks\n",
    "# Split it into multiple samples using overlapping chunks\n",
    "inputs = tokenizer(\n",
    "  question,\n",
    "  context,\n",
    "  max_length=100, # max length of the string\n",
    "  truncation=\"only_second\", # only chunk/truncate second string which is the context\n",
    "  stride=50, # overlap between chunks\n",
    "  # better name for return_overflowing_tokens would be return_overlapping_tokens \n",
    "  return_overflowing_tokens=True, # Set to True, will chunk other tokens beyond the max_length\n",
    "  return_offsets_mapping=True  \n",
    ")\n",
    "\n",
    "# Print the chunks inputs for the 1 data row\n",
    "# each chunk token id will start for 101(for CLS) and end with 102 (for SEP)\n",
    "print(\"Chunked context ids:\",inputs[\"input_ids\"])\n",
    "\n",
    "# Decode the individual token ids\n",
    "# Note the decoded question will be same in all cases\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "  print(\"Decoded and chunked tokens:\",tokenizer.decode(ids))\n",
    "\n",
    "# Format is [CLS] question [SEP] context [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the new keys overflow_to_sample_mapping and offset_mapping\n",
    "\n",
    "overflow_to_sample_mapping: will contain [0, 0, 0, 1, 1, 2, 2] which shows how input data is split into multiple samples\n",
    "\n",
    "[0, 0, 0, 1, 1, 2, 2] it will show how first sample [0] is split into 3 samples, [1] is split into 2 samples and [2] is split into 1, 2 are\n",
    "split into 2 samples\n",
    "\n",
    "offset_mapping: Shows offset/character positions of tokens in the mapping. For example, if the decoded tokens are:\n",
    "\n",
    "[CLS] What is in front of the Notre Dame Main Building?\n",
    "\n",
    "The offset mapping will be like: [(0, 0), (0, 4), (5, 7), (8, 10), ...\n",
    "\n",
    "(0,0) : for CLS\n",
    "(0,4) : For 'What', start from 0 and go till offset 4 as length is 4\n",
    "(5,7) : For 'is', start from 5 and go till offset 7 as length is 2\n",
    "(8,10): For 'in', start from 8 and go till offset 10 as length is 2\n",
    "\n",
    "Note this list restart from (0,0) for a new chunk, but the index still refers to the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.keys())\n",
    "inputs['overflow_to_sample_mapping']\n",
    "\n",
    "# Output will be (0,0,0,0), which show that all the 4 chunks belong to the same data row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each SEP token will show up as (0,0) in this\n",
    "inputs['offset_mapping'] # Shows index of each token in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For out example, there are 4 inputs chunks, and each chunk has question and chunked context\n",
    "# the sequence id will be 0 for question and 1 for context, for each of the chunk ids passed to it \n",
    "# this is similar to token type ids\n",
    "\n",
    "inputs.sequence_ids(0)  # Shows the sequence id of each token for chunk 0\n",
    "\n",
    "# NOTE: inputs.sequence_ids(4) will throw an error as there are only 4 chunks\n",
    "# SEP and CLS tokens will be shown as none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = raw_datasets[\"train\"][1][\"answers\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index in sequence_id where context starts\n",
    "# Remember that the sequence_id is 0 for question and 1 for context\n",
    "\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "\n",
    "#x:y:z in Python means to start at x, end at y, and step by z \n",
    "# find where sequence_id changes from 0 to 1\n",
    "ctx_start = sequence_ids.index(1) # .index() will return the first index where the value is 1\n",
    "# sequence_ids[::-1].index(1) will returns from the other side of the list where the value is 1\n",
    "idx = sequence_ids[::-1].index(1) # will return 1 => second index from right where 1 starts \n",
    "ctx_end= len(sequence_ids) - idx - 1 # find the last index where the value is 1\n",
    "# Example: sequence_ids = [0,0,1,1,1,1,None]\n",
    "# ctx_start = 2, ctx_end = 7 - 1 - 1 = 5\n",
    "ctx_start, ctx_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether or not the answer is fully contained within the context\n",
    "# if not, target is (start, end) = (0, 0)\n",
    "print(\"answer:\",answer)\n",
    "\n",
    "ans_start_char = answer['answer_start'][0] # location in first index, will return 188 for the sample\n",
    "ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "offset = inputs['offset_mapping'][0] #[0] give first question and context chunk\n",
    "print(\"offset:\",offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer is provided in terms of character positions in the context\n",
    "# However for neurl network, we need to provide the answer in terms of token positions\n",
    "# This function will find the token positions of the answer in the context\n",
    "def find_answer_token_idx(\n",
    "    ctx_start,\n",
    "    ctx_end,\n",
    "    ans_start_char,\n",
    "    ans_end_char,\n",
    "    offset):\n",
    "  \n",
    "  start_idx = 0\n",
    "  end_idx = 0\n",
    "\n",
    "  if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "    pass\n",
    "    # print(\"target is (0, 0)\")\n",
    "    # nothing else to do\n",
    "  else:\n",
    "    # find the start and end TOKEN positions\n",
    "\n",
    "    # the 'trick' is knowing what is in units of tokens and what is in\n",
    "    # units of characters\n",
    "\n",
    "    # recall: the offset_mapping contains the character positions of each token\n",
    "\n",
    "    i = ctx_start\n",
    "    for start_end_char in offset[ctx_start:]:\n",
    "      start, end = start_end_char\n",
    "      if start == ans_start_char:\n",
    "        start_idx = i\n",
    "        # don't break yet\n",
    "      \n",
    "      if end == ans_end_char:\n",
    "        end_idx = i\n",
    "        break\n",
    "\n",
    "      i += 1\n",
    "  return start_idx, end_idx\n",
    "\n",
    "# Token positions where the answer starts and ends\n",
    "start_idx, end_idx = find_answer_token_idx(ctx_start, ctx_end, ans_start_char, ans_end_char, offset)\n",
    "\n",
    "print (f\"start_idx, end_idx: {start_idx, end_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the values of the answers based on the start_idx and end_idx\n",
    "# Check the Token ids\n",
    "input_ids = inputs['input_ids'][0]\n",
    "print(\"Token ids for answer\",input_ids[start_idx : end_idx + 1])\n",
    "\n",
    "# Decoded values of the tokens\n",
    "print(\"Decoded Values:\",tokenizer.decode(input_ids[start_idx : end_idx + 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the process of tokenizing the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenize function for the entire batch which will be called from the map function\n",
    "\n",
    "# Use these values as Google used 384 for SQuAD\n",
    "max_length = 384\n",
    "stride =  128\n",
    "\n",
    "# This function is used only for the train data\n",
    "def tokenize_fn_train(batch):\n",
    "  # some questions have leading and/or trailing whitespace\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them from the dict\n",
    "  # offset mapping will have the question first followed by the chunked context \n",
    "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\") # Shows which chunk belongs to which sample\n",
    "  # e.g. orig_sample_idxs = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
    "  # => Chunk 0-3 belongs to sample 0, Chunk 4-7 belongs to sample 1, Chunk 8-11 belongs to sample 2\n",
    "  answers = batch['answers']\n",
    "  start_idxs, end_idxs = [], []\n",
    "\n",
    "  # Put the start and end position of the answers\n",
    "  # in the end positions, we will use the function defined previously\n",
    "  # offset_mapping = [[(0,1),(2,5)...],[(0,6),(7,12)...],...]\n",
    "  for i, offset in enumerate(offset_mapping): # i, offset =   0, [(0,1),(2,5)...] \n",
    "    sample_idx = orig_sample_idxs[i] # Sample index will be sample for multiple chunks of same sample\n",
    "    \n",
    "    # Searching for the answer in the specific context\n",
    "    answer = answers[sample_idx]\n",
    "    ans_start_char = answer['answer_start'][0]\n",
    "    ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # find start + end of context (first 1 and last 1)\n",
    "    # We will find if the answer is in this chunked context or not\n",
    "    ctx_start = sequence_ids.index(1)\n",
    "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "    start_idx, end_idx = find_answer_token_idx(\n",
    "      ctx_start,\n",
    "      ctx_end,\n",
    "      ans_start_char,\n",
    "      ans_end_char,\n",
    "      offset)\n",
    "\n",
    "    # Note that due to stride the answer can appear in multiple context \n",
    "    # windows\n",
    "    start_idxs.append(start_idx) # if start_idx = end_idx = 0, then answer is not in the context\n",
    "    end_idxs.append(end_idx)\n",
    "  \n",
    "  # Add new fields in the input.\n",
    "  inputs[\"start_positions\"] = start_idxs\n",
    "  inputs[\"end_positions\"] = end_idxs\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train dataset\n",
    "# Use the mapping functions to tokenize the data\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "  tokenize_fn_train,\n",
    "  batched=True,\n",
    "  # Will remove all columns present in original data in the new dataset\n",
    "  # This will insure that none of the original columns are present in the new dataset\n",
    "  # as they are not used \n",
    "  remove_columns=raw_datasets[\"train\"].column_names, \n",
    ")\n",
    "\n",
    "# remove_columns will remove the columns from the dataset\n",
    "# See the difference in the length of the raw dataset and the tokenized dataset\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do data prep for validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one Sample\n",
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the validation set differently\n",
    "# we won't need the targets since we will just compare with the original answer\n",
    "# also: overwrite offset_mapping with Nones in place of question\n",
    "# More details on this: https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "def tokenize_fn_validation(batch):\n",
    "  # some questions have leading and/or trailing whitespace, strip them\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them\n",
    "  # keep the offset mapping as it will be used to find the answer\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "  sample_ids = []\n",
    "\n",
    "  # rewrite offset mapping by replacing question tuples with None\n",
    "  # this will be helpful later on when we compute metrics\n",
    "  for i in range(len(inputs[\"input_ids\"])):\n",
    "    sample_idx = orig_sample_idxs[i]\n",
    "    sample_ids.append(batch['id'][sample_idx])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    offset = inputs[\"offset_mapping\"][i]\n",
    "    # Change any value that does not belong to the context to None\n",
    "    # Remember that the sequence_id is 0 for question and 1 for context\n",
    "    inputs[\"offset_mapping\"][i] = [\n",
    "      x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)]\n",
    "    \n",
    "  inputs['sample_id'] = sample_ids\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the validation dataset using map function defined earlier\n",
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "  tokenize_fn_validation,\n",
    "  batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "# The length will differ as chunking will create additional samples\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build code for the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- This is not used anymore\n",
    "# from datasets import load_metric\n",
    "# metric = load_metric(\"squad\")\n",
    "#------------------------\n",
    "#pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "# Most standards datasets for NLP tasks have associated metrics for them\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a sample structures of predicted and true answers, and how they are passed to the compute function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers = [\n",
    "  {'id': '1', 'prediction_text': 'Albert Einstein'},\n",
    "  {'id': '2', 'prediction_text': 'physicist'},\n",
    "  {'id': '3', 'prediction_text': 'general relativity'},\n",
    "]\n",
    "true_answers = [\n",
    "  {'id': '1', 'answers': {'text': ['Albert Einstein'], 'answer_start': [100]}},\n",
    "  {'id': '2', 'answers': {'text': ['physicist'], 'answer_start': [100]}},\n",
    "  {'id': '3', 'answers': {'text': ['special relativity'], 'answer_start': [100]}},\n",
    "]\n",
    "\n",
    "# id and answer_start seem superfluous but you'll get an error if not included\n",
    "# metrics.compute will give accuracy and F1 score\n",
    "metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a smaller validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next problem: how to go from logits to prediction text?\n",
    "small_validation_dataset = raw_datasets[\"validation\"].select(range(100)) # select 1 to 100\n",
    "\n",
    "# Let's work on an already-trained question-answering model\n",
    "# Model name will be used in both AutoTokenizer and AutoModelForQuestionAnswering\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "# temporarily assign tokenizer2 to tokenizer since it's used as a global\n",
    "# in tokenize_fn_validation. The original tokenizer is declared earlier in the code\n",
    "old_tokenizer = tokenizer\n",
    "tokenizer = tokenizer2\n",
    "\n",
    "small_validation_processed = small_validation_dataset.map(\n",
    "    tokenize_fn_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# change it back\n",
    "tokenizer = old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the definition of the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model prepped for training\n",
    "\n",
    "# get the model outputs\n",
    "import torch\n",
    "# AutoModelForQuestionAnswering will be downloaded from the Hugging Face model hub\n",
    "# based on the model name we had specified earlier\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# the trained model doesn't use these columns\n",
    "small_model_inputs = small_validation_processed.remove_columns(\n",
    "  [\"sample_id\", \"offset_mapping\"])\n",
    "small_model_inputs.set_format(\"torch\")\n",
    "\n",
    "# get gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")  # should be cuda if GPU is available\n",
    "\n",
    "# move tensors to gpu device using .to(device)\n",
    "small_model_inputs_gpu = {\n",
    "  k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names\n",
    "}\n",
    "\n",
    "# download the model\n",
    "# Note: This model is already pretrained on QA tasks\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "  trained_checkpoint).to(device)\n",
    "\n",
    "# get the model outputs\n",
    "# This will be used to get the logits before the training\n",
    "with torch.no_grad():\n",
    "  outputs = trained_model(**small_model_inputs_gpu)\n",
    "\n",
    "# The logits will be of size N x T  \n",
    "print(f\"Entries in Output: {outputs.keys()})\")\n",
    "\n",
    "# outputs if of format: QuestionAnsweringModelOutput\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Orig Sample IDs\n",
    "print(\"Sample IDs:\",validation_dataset['sample_id'])\n",
    "\n",
    "# However if the context is too long, the answer may not be in the context so the sample ID\n",
    "# will repeat. Therefore if we take the unique values of the sample ID, it will be lower than the sample\n",
    "# IDs taken for the full dataset, as seen below:\n",
    "\n",
    "len(set(validation_dataset['sample_id']))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert outputs fron DNN back to a string output, as metric inputs is a string. String also will helps the human understand the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the start_logits and end_logits give the logits for the start and end of the answer\n",
    "# within the context. \n",
    "# Note that the start_logits and end_logits are used only in this model type\n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "# Shape is 100 x 384\n",
    "# 100 is the number of samples selected from the main validation set \n",
    "# 384 is the max length of the input specified in the function used in the map function\n",
    "# Hence logits gives the propobability for each token to be the start and end of the answer\n",
    "start_logits.shape, end_logits.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start, end: 22331, 22332\n"
     ]
    }
   ],
   "source": [
    "# Get the highest probability from the model output for the start and end positions:\n",
    "# This method is not robust as sometimes answer_end_index can be less than answer_start_index\n",
    "# a different method is used in the next cell\n",
    "# argmax() returns tensor of indices of the maximum value of all elements in the input tensor\n",
    "answer_start_index = outputs.start_logits.argmax(keepdim=False)\n",
    "answer_end_index = outputs.end_logits.argmax(keepdim=False)\n",
    "\n",
    "# The start and end positions are shown for the entire index (row is added to the column)\n",
    "print(f\"Start, end: {answer_start_index}, {answer_end_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different approach to convert the logits to the text is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56be4db0acb8001400a502ec',\n",
       " '56be4db0acb8001400a502ed',\n",
       " '56be4db0acb8001400a502ee',\n",
       " '56be4db0acb8001400a502ef',\n",
       " '56be4db0acb8001400a502f0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the sample of sample_ids, this is a unique id for each sample\n",
    "small_validation_processed['sample_id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping function that maps the sample id to its actual index in the data structure\n",
    "# example: {'56be4db0acb8001400a502ec': [0, 1, 2, 3], ...}\n",
    "sample_id2idxs = {}\n",
    "for i, id_ in enumerate(small_validation_processed['sample_id']):\n",
    "  if id_ not in sample_id2idxs:\n",
    "    sample_id2idxs[id_] = [i]\n",
    "  else:\n",
    "    sample_id2idxs[id_].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in descending order: [360 361 362 364 370 365 359 354 374 356 363 357 373 358 222 328 280 383\n",
      " 355 198 216 369 375 206 381 219 242 366 371 220 380 231 376 221 353 379\n",
      " 352 382 368 247 367 215 344 235 207 241 244 372 217 243 224 226 340 351\n",
      " 180 184 349 202 199 330 193 223 262 203 347 204 236 282 211 225 200 205\n",
      " 201 208 263 378 213 197 339 188 248 195 196 258 377 182 257 194 246 249\n",
      " 240 261 336 209 348 212 277 324 210 228 178 275 341 260 189 183 335 181\n",
      " 343 345 342 279 271 254 185 268 281 259 350 278 325 192 214 334 253 238\n",
      " 329 267 172 332 239 187 173 273 285 179 232 218 230 191 346 233 272 276\n",
      " 326 255 284 227 237 234 327 331 288 264 286 293 190 266 337 256 174 283\n",
      " 250 245 186 297 302 177 229 296 252 290 322 323 292 338 175 176 298 265\n",
      " 274 299 269 303 333 251 294 289 295 315 270 287 305 301 318 291 321 319\n",
      " 320 317 306 314 308 316 307 300 310 313 309 304 312 311 122 129 149 147\n",
      " 121 113 146 125 158 142 120  99 116 161 163 148 126 168 167 156 135 138\n",
      " 162 119 145 133 155 157 140 104 127 117 154 170 152 134 112 128 141  94\n",
      " 160 139 144   9  91 131 159 166 130 137 164 143 114 107  85  82 118  83\n",
      " 153 124 150 123  77 132  25  88   4 108  89 111  72  86 102 103   7  97\n",
      "   5  60 115   3  90   8 109  67 165  93 100 106 169 136  96  10  98 105\n",
      "  79  80  16   6   2  74   1 151  84 110  92  73  71  76  12 171  87  68\n",
      "  33  22  14  17  20  19  21  63  30  95  34  66  62 101  11  32  36  23\n",
      "  64  61  75  28  29  69  26  55  51  18  70  81  65  24  52   0  15  78\n",
      "  59  37  44  53  41  48  31  27  35  40  42  13  49  56  54  45  43  50\n",
      "  58  39  38  47  57  46]\n",
      "Values in ascending order: [ 46  57  47  38  39  58  50  43  45  54  56  49  13  42  40  35  27  31\n",
      "  48  41  53  44  37  59  78  15   0  52  24  65  81  70  18  51  55  26\n",
      "  69  29  28  75  61  64  23  36  32  11 101  62  66  34  95  30  63  21\n",
      "  19  20  17  14  22  33  68  87 171  12  76  71  73  92 110  84 151   1\n",
      "  74   2   6  16  80  79 105  98  10  96 136 169 106 100  93 165  67 109\n",
      "   8  90   3 115  60   5  97   7 103 102  86  72 111  89 108   4  88  25\n",
      " 132  77 123 150 124 153  83 118  82  85 107 114 143 164 137 130 166 159\n",
      " 131  91   9 144 139 160  94 141 128 112 134 152 170 154 117 127 104 140\n",
      " 157 155 133 145 119 162 138 135 156 167 168 126 148 163 161 116  99 120\n",
      " 142 158 125 146 113 121 147 149 129 122 311 312 304 309 313 310 300 307\n",
      " 316 308 314 306 317 320 319 321 291 318 301 305 287 270 315 295 289 294\n",
      " 251 333 303 269 299 274 265 298 176 175 338 292 323 322 290 252 296 229\n",
      " 177 302 297 186 245 250 283 174 256 337 266 190 293 286 264 288 331 327\n",
      " 234 237 227 284 255 326 276 272 233 346 191 230 218 232 179 285 273 173\n",
      " 187 239 332 172 267 329 238 253 334 214 192 325 278 350 259 281 268 185\n",
      " 254 271 279 342 345 343 181 335 183 189 260 341 275 178 228 210 324 277\n",
      " 212 348 209 336 261 240 249 246 194 257 182 377 258 196 195 248 188 339\n",
      " 197 213 378 263 208 201 205 200 225 211 282 236 204 347 203 262 223 193\n",
      " 330 199 202 349 184 180 351 340 226 224 243 217 372 244 241 207 235 344\n",
      " 215 367 247 368 382 352 379 353 221 376 231 380 220 371 366 242 219 381\n",
      " 206 375 369 216 198 355 383 280 328 222 358 373 357 363 356 374 354 359\n",
      " 365 370 364 362 361 360]\n"
     ]
    }
   ],
   "source": [
    "# argsort() returns the indices of values in sorted order\n",
    "print(\"Values in descending order:\",start_logits[0].argsort())\n",
    "# Convert values to -ve to get the values in ascending order\n",
    "print(\"Values in ascending order:\",(-start_logits[0]).argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.694443  ,  9.803684  ,  4.459974  ,  4.400486  ,  2.9437783 ,\n",
       "        2.7017372 ,  2.012644  ,  1.5780748 ,  0.5223746 ,  0.02073722,\n",
       "       -0.02802708, -0.04971639, -0.38573125, -0.6945367 , -0.79795045,\n",
       "       -0.8678042 , -0.87220824, -1.3516885 , -1.3703709 , -1.3878822 ,\n",
       "       -1.5135099 , -1.735547  , -1.8827038 , -1.8932867 , -1.9078968 ,\n",
       "       -1.9304981 , -2.2607315 , -2.2983897 , -2.306934  , -2.502741  ,\n",
       "       -2.510062  , -2.5308425 , -2.539996  , -2.6718142 , -2.7323534 ,\n",
       "       -2.7710214 , -2.7713675 , -2.9521344 , -3.0604677 , -3.1706069 ,\n",
       "       -3.204545  , -3.569337  , -3.5798054 , -3.6668842 , -3.7250612 ,\n",
       "       -3.7498565 , -3.7632205 , -3.996813  , -4.01133   , -4.0688014 ,\n",
       "       -4.0944867 , -4.1954756 , -4.238311  , -4.332363  , -4.352416  ,\n",
       "       -4.3879642 , -4.388612  , -4.396614  , -4.6790533 , -4.7030315 ,\n",
       "       -4.775757  , -4.777815  , -4.7882195 , -4.788246  , -4.8221292 ,\n",
       "       -4.8725405 , -4.8849354 , -4.8981485 , -5.072099  , -5.1078773 ,\n",
       "       -5.14864   , -5.178327  , -5.1912117 , -5.2709    , -5.3146415 ,\n",
       "       -5.377077  , -5.5316544 , -5.571131  , -5.649819  , -5.661308  ,\n",
       "       -5.6778765 , -5.6986113 , -5.75157   , -5.8426423 , -5.8621535 ,\n",
       "       -5.907693  , -5.9093795 , -5.9444513 , -5.996892  , -6.005831  ,\n",
       "       -6.047035  , -6.0640106 , -6.0858793 , -6.100561  , -6.2241745 ,\n",
       "       -6.2670965 , -6.2752934 , -6.3032966 , -6.3134212 , -6.328803  ,\n",
       "       -6.3306904 , -6.4014864 , -6.4204845 , -6.43611   , -6.4374104 ,\n",
       "       -6.450714  , -6.531434  , -6.5530467 , -6.563021  , -6.6687956 ,\n",
       "       -6.7266917 , -6.742634  , -6.744299  , -6.745859  , -6.810875  ,\n",
       "       -6.921632  , -6.949301  , -6.9907675 , -6.9987483 , -7.16226   ,\n",
       "       -7.1673565 , -7.17566   , -7.1805634 , -7.196786  , -7.243598  ,\n",
       "       -7.273346  , -7.27695   , -7.3002214 , -7.300935  , -7.3218613 ,\n",
       "       -7.397678  , -7.433037  , -7.5248475 , -7.527233  , -7.532275  ,\n",
       "       -7.5737123 , -7.598215  , -7.6580987 , -7.659709  , -7.715575  ,\n",
       "       -7.718405  , -7.7400184 , -7.7546535 , -7.885648  , -7.8937674 ,\n",
       "       -7.894373  , -7.938037  , -7.9582057 , -8.037517  , -8.059219  ,\n",
       "       -8.0639105 , -8.104632  , -8.117334  , -8.138487  , -8.145781  ,\n",
       "       -8.221222  , -8.27425   , -8.292226  , -8.293207  , -8.316831  ,\n",
       "       -8.317497  , -8.334001  , -8.466509  , -8.500583  , -8.502035  ,\n",
       "       -8.591037  , -8.61247   , -8.621495  , -8.625213  , -8.625459  ,\n",
       "       -8.814262  , -9.039057  , -9.421058  , -9.434795  , -9.441469  ,\n",
       "       -9.444378  , -9.449848  , -9.450176  , -9.450437  , -9.451044  ,\n",
       "       -9.4520645 , -9.452425  , -9.452515  , -9.454132  , -9.456528  ,\n",
       "       -9.458106  , -9.458341  , -9.458443  , -9.458752  , -9.460876  ,\n",
       "       -9.462076  , -9.46251   , -9.462946  , -9.463791  , -9.463894  ,\n",
       "       -9.465527  , -9.466454  , -9.468564  , -9.468857  , -9.468961  ,\n",
       "       -9.469493  , -9.46952   , -9.470061  , -9.470967  , -9.471138  ,\n",
       "       -9.471288  , -9.47136   , -9.471789  , -9.471895  , -9.472049  ,\n",
       "       -9.472687  , -9.472961  , -9.473713  , -9.475199  , -9.47661   ,\n",
       "       -9.476766  , -9.47678   , -9.477411  , -9.478312  , -9.478872  ,\n",
       "       -9.47975   , -9.479919  , -9.480068  , -9.48065   , -9.481191  ,\n",
       "       -9.48147   , -9.481522  , -9.4815235 , -9.481611  , -9.48179   ,\n",
       "       -9.482164  , -9.482849  , -9.483416  , -9.484571  , -9.484709  ,\n",
       "       -9.484896  , -9.485175  , -9.485302  , -9.485718  , -9.485962  ,\n",
       "       -9.486804  , -9.486973  , -9.487631  , -9.487732  , -9.487915  ,\n",
       "       -9.487995  , -9.488298  , -9.488397  , -9.48859   , -9.488804  ,\n",
       "       -9.488961  , -9.489101  , -9.4894905 , -9.4896755 , -9.490001  ,\n",
       "       -9.490805  , -9.490836  , -9.490873  , -9.4909    , -9.4911375 ,\n",
       "       -9.4912615 , -9.491712  , -9.492028  , -9.4921665 , -9.492537  ,\n",
       "       -9.492821  , -9.493561  , -9.493635  , -9.493653  , -9.494005  ,\n",
       "       -9.49403   , -9.494392  , -9.495206  , -9.495425  , -9.496534  ,\n",
       "       -9.496603  , -9.497137  , -9.49753   , -9.497562  , -9.497578  ,\n",
       "       -9.497606  , -9.497616  , -9.498016  , -9.498124  , -9.498629  ,\n",
       "       -9.498648  , -9.49877   , -9.499588  , -9.500017  , -9.500347  ,\n",
       "       -9.5004225 , -9.500581  , -9.500978  , -9.500999  , -9.50117   ,\n",
       "       -9.501221  , -9.50128   , -9.501437  , -9.501508  , -9.501884  ,\n",
       "       -9.50201   , -9.5025215 , -9.502687  , -9.503566  , -9.503639  ,\n",
       "       -9.503661  , -9.504168  , -9.504551  , -9.505049  , -9.505533  ,\n",
       "       -9.506172  , -9.5066395 , -9.50668   , -9.506757  , -9.506837  ,\n",
       "       -9.507122  , -9.507558  , -9.507732  , -9.507991  , -9.508051  ,\n",
       "       -9.508085  , -9.508731  , -9.508774  , -9.508862  , -9.509275  ,\n",
       "       -9.509516  , -9.509531  , -9.510824  , -9.511493  , -9.5120325 ,\n",
       "       -9.512429  , -9.512821  , -9.513138  , -9.513152  , -9.513192  ,\n",
       "       -9.513242  , -9.513386  , -9.514933  , -9.515438  , -9.515572  ,\n",
       "       -9.51619   , -9.516241  , -9.516349  , -9.516382  , -9.51666   ,\n",
       "       -9.517018  , -9.518312  , -9.518468  , -9.518797  , -9.519094  ,\n",
       "       -9.520803  , -9.521006  , -9.521193  , -9.521229  , -9.522541  ,\n",
       "       -9.522743  , -9.522884  , -9.523234  , -9.523897  , -9.52433   ,\n",
       "       -9.525376  , -9.526042  , -9.527031  , -9.52847   , -9.528532  ,\n",
       "       -9.5286255 , -9.528761  , -9.52946   , -9.529714  , -9.53137   ,\n",
       "       -9.532367  , -9.532747  , -9.535541  , -9.536648  , -9.537714  ,\n",
       "       -9.538664  , -9.539051  , -9.539339  , -9.540269  , -9.541707  ,\n",
       "       -9.548329  , -9.55448   , -9.557686  , -9.567406  ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the actial value from the sorted index\n",
    "start_logits[0][(-start_logits[0]).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset Map for single train data (in index 0)\n",
      " [None, None, None, None, None, None, None, None, None, None, None, None, None, [0, 5], [6, 10], [11, 13], [14, 17], [18, 20], [21, 29], [30, 38], [39, 43], [44, 46], [47, 56], [57, 60], [61, 69], [70, 72], [73, 76], [77, 85], [86, 94], [95, 101], [102, 103], [103, 106], [106, 107], [108, 111], [112, 115], [116, 120], [121, 127], [127, 128], [129, 132], [133, 141], [142, 150], [151, 161], [162, 163], [163, 166], [166, 167], [168, 176], [177, 183], [184, 191], [192, 200], [201, 204], [205, 213], [214, 222], [223, 233], [234, 235], [235, 238], [238, 239], [240, 248], [249, 257], [258, 266], [267, 269], [269, 270], [270, 272], [273, 275], [276, 280], [281, 286], [287, 292], [293, 298], [299, 303], [304, 309], [309, 310], [311, 314], [315, 319], [320, 323], [324, 330], [331, 333], [334, 342], [343, 344], [344, 345], [346, 350], [350, 351], [352, 354], [355, 359], [359, 360], [360, 361], [362, 369], [370, 372], [373, 376], [377, 380], [381, 390], [391, 394], [395, 399], [400, 402], [403, 408], [409, 414], [414, 415], [416, 426], [426, 427], [428, 430], [431, 435], [436, 439], [440, 443], [444, 448], [449, 454], [455, 459], [459, 460], [461, 464], [465, 471], [472, 482], [483, 486], [487, 488], [488, 494], [495, 506], [506, 507], [508, 512], [513, 520], [521, 525], [525, 526], [526, 532], [533, 544], [544, 545], [546, 548], [549, 553], [554, 556], [557, 568], [569, 571], [571, 573], [573, 579], [580, 583], [584, 593], [594, 596], [597, 603], [604, 608], [609, 614], [615, 619], [620, 624], [625, 629], [630, 635], [636, 637], [637, 640], [640, 644], [645, 646], [646, 651], [652, 657], [658, 661], [662, 666], [667, 672], [673, 677], [678, 682], [683, 688], [689, 691], [692, 693], [693, 698], [699, 703], [704, 705], [705, 706], [706, 707], [707, 708], [709, 711], [712, 716], [717, 720], [721, 725], [726, 731], [732, 743], [744, 751], [752, 755], [756, 762], [763, 764], [764, 767], [767, 771], [772, 774], [774, 775], None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Single Offset Map value for single train data (in index 0) and char index 20\n",
      " [39, 43]\n",
      "Start Value from Single Offset Map value for single train data (in index 0) and char index 20\n",
      " 39\n",
      "End Value from Single Offset Map value for single train data (in index 0) and char index 20\n",
      " 43\n"
     ]
    }
   ],
   "source": [
    "# Note: Earlier we made a change in offset_mapping where we stored None \n",
    "# everywhere except the context window\n",
    "# in the context window we store tuples for each token containing:\n",
    "# (start_character_position, end_character_position)\n",
    "print(\"Offset Map for single train data (in index 0)\\n\", small_validation_processed[0]['offset_mapping'])\n",
    "\n",
    "print(\"Single Offset Map value for single train data (in index 0) and char index 20\\n\", small_validation_processed[0]['offset_mapping'][20])\n",
    "\n",
    "print(\"Start Value from Single Offset Map value for single train data (in index 0) and char index 20\\n\", small_validation_processed[0]['offset_mapping'][20][0])\n",
    "\n",
    "print(\"End Value from Single Offset Map value for single train data (in index 0) and char index 20\\n\", small_validation_processed[0]['offset_mapping'][20][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
